{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I followed the below blogpage until the implamentation of the mode: \n",
    "\n",
    "I had to add one additional dimension to the images for 'batch' that SegFormer model needs. That is why matplotlib functions will give an error (just skip for now). The ultimate error is for the last cell. This error occurs because the proj layer expects the input tensor to have a shape with the last dimension (axis -1) equal to 3, but the input shape received by the proj layer is (1, 134, 9, 128).\n",
    "\n",
    "The issue seems to be related to the data preprocessing or input configuration for your model. To fix this, you should check the following:\n",
    "\n",
    "Verify that the images in your dataset have the correct shape (height, width, channels). In this case, the channels should be 3, as it represents the RGB color channels.\n",
    "Check the data preprocessing pipeline for your images. Ensure that the images are being loaded and resized correctly to the expected input shape of the model.\n",
    "Verify the configuration of the proj layer or any other layer in your segformer model that might expect a specific input shape. Make sure the input shape is compatible with the model architecture.\n",
    "Ensure that the input images are correctly passed to the model during training and validation.\n",
    "\n",
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2023/04/deep-learning-for-image-segmentation-with-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# A list to collect paths of 1000 images\n",
    "image_path = []\n",
    "for root, dirs, files in os.walk('/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/data/xBD_test_subset_raw_reorganized/images'):\n",
    "    # Iterate over 1000 images\n",
    "    for file in files:\n",
    "        # Check if the file has a PNG extension\n",
    "        if file.lower().endswith('.png'):\n",
    "            # Create path\n",
    "            path = os.path.join(root, file)\n",
    "            # Add path to list\n",
    "            image_path.append(path)\n",
    "print(len(image_path))\n",
    "\n",
    "# A list to collect paths of 1000 masks\n",
    "mask_path = []\n",
    "for root, dirs, files in os.walk('/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/data/xBD_test_subset_raw_reorganized/masks'):\n",
    "    # Iterate over 1000 masks\n",
    "    for file in files:\n",
    "        # Check if the file has a PNG extension\n",
    "        if file.lower().endswith('.png'):\n",
    "            # Obtain the path\n",
    "            path = os.path.join(root, file)\n",
    "            # Add path to the list\n",
    "            mask_path.append(path)\n",
    "print(len(mask_path))\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:02<00:00, 43.31it/s]\n",
      "100%|██████████| 110/110 [00:01<00:00, 89.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# create a list to store images\n",
    "images = []\n",
    "# iterate over 1000 image paths\n",
    "for path in tqdm(image_path):\n",
    "    # read file\n",
    "    file = tf.io.read_file(path)\n",
    "    # decode png file into a tensor\n",
    "    image = tf.image.decode_png(file, channels=3, dtype=tf.uint8)\n",
    "\n",
    "    #adding 4th dimension for batch size \n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    # append to the list\n",
    "    images.append(image)\n",
    "\n",
    "\n",
    "# create a list to store masks\n",
    "masks = []\n",
    "# iterate over 1000 mask paths\n",
    "for path in tqdm(mask_path):\n",
    "    # read the file\n",
    "    file = tf.io.read_file(path)\n",
    "    # decode png file into a tensor\n",
    "    mask = tf.image.decode_png(file, channels=3, dtype=tf.uint8)\n",
    "\n",
    "    #adding 4th dimension for batch size \n",
    "    mask = tf.expand_dims(mask, axis=0)\n",
    "    # append mask to the list\n",
    "    masks.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the first image\n",
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 1024, 1024, 3) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m img \u001b[39m=\u001b[39m images[i]\n\u001b[1;32m      9\u001b[0m \u001b[39m# Show the image with a colorbar\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(img)\n\u001b[1;32m     11\u001b[0m plt\u001b[39m.\u001b[39mcolorbar()\n\u001b[1;32m     12\u001b[0m \u001b[39m# Turn off the axis labels\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/_api/deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[1;32m    454\u001b[0m     warn_deprecated(\n\u001b[1;32m    455\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    458\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/pyplot.py:2652\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[1;32m   2647\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[1;32m   2648\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2649\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[1;32m   2650\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[1;32m   2651\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2652\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[1;32m   2653\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[1;32m   2654\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[1;32m   2655\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[1;32m   2656\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n\u001b[1;32m   2657\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m   2658\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   2659\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2660\u001b[0m     sci(__ret)\n\u001b[1;32m   2661\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/_api/deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[1;32m    454\u001b[0m     warn_deprecated(\n\u001b[1;32m    455\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    458\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1411\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1414\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1415\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1416\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5481\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5474\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5475\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap, norm, interpolation,\n\u001b[1;32m   5476\u001b[0m                       origin, extent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5477\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5478\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5479\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5481\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5482\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5483\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5484\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/image.py:715\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    714\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[0;32m--> 715\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    716\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[1;32m    718\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    719\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 1024, 1024, 3) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2500x1300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(25,13))\n",
    "\n",
    "# Iterate over the images in the range 4-6\n",
    "for i in range(4,7):\n",
    "    # Create a subplot for each image\n",
    "    plt.subplot(4,6,i)\n",
    "    # Get the i-th image from the list\n",
    "    img = images[i]\n",
    "    # Show the image with a colorbar\n",
    "    plt.imshow(img)\n",
    "    plt.colorbar()\n",
    "    # Turn off the axis labels\n",
    "    plt.axis('off')\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 1024, 1024, 3) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[69], line 9\u001b[0m\n",
      "\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m4\u001b[39m,\u001b[39m6\u001b[39m,i)\n",
      "\u001b[1;32m      8\u001b[0m img \u001b[39m=\u001b[39m masks[i]\n",
      "\u001b[0;32m----> 9\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(img, cmap\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mjet\u001b[39;49m\u001b[39m'\u001b[39;49m, norm\u001b[39m=\u001b[39;49mNORM)\n",
      "\u001b[1;32m     10\u001b[0m plt\u001b[39m.\u001b[39mcolorbar()\n",
      "\u001b[1;32m     11\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/_api/deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n",
      "\u001b[1;32m    454\u001b[0m     warn_deprecated(\n",
      "\u001b[1;32m    455\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[1;32m    458\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/pyplot.py:2652\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   2646\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n",
      "\u001b[1;32m   2647\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n",
      "\u001b[1;32m   2648\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n",
      "\u001b[1;32m   2649\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n",
      "\u001b[1;32m   2650\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n",
      "\u001b[1;32m   2651\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[0;32m-> 2652\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n",
      "\u001b[1;32m   2653\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n",
      "\u001b[1;32m   2654\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n",
      "\u001b[1;32m   2655\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n",
      "\u001b[1;32m   2656\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n",
      "\u001b[1;32m   2657\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n",
      "\u001b[1;32m   2658\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n",
      "\u001b[1;32m   2659\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   2660\u001b[0m     sci(__ret)\n",
      "\u001b[1;32m   2661\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/_api/deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n",
      "\u001b[1;32m    454\u001b[0m     warn_deprecated(\n",
      "\u001b[1;32m    455\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[1;32m    458\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1409\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n",
      "\u001b[1;32m   1410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[1;32m   1411\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m-> 1412\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1414\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32m   1415\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n",
      "\u001b[1;32m   1416\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5481\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   5474\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n",
      "\u001b[1;32m   5475\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap, norm, interpolation,\n",
      "\u001b[1;32m   5476\u001b[0m                       origin, extent, filternorm\u001b[39m=\u001b[39mfilternorm,\n",
      "\u001b[1;32m   5477\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n",
      "\u001b[1;32m   5478\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n",
      "\u001b[1;32m   5479\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m-> 5481\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n",
      "\u001b[1;32m   5482\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n",
      "\u001b[1;32m   5483\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m   5484\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/image.py:715\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n",
      "\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n",
      "\u001b[1;32m    714\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n",
      "\u001b[0;32m--> 715\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    716\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n",
      "\u001b[1;32m    718\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n",
      "\u001b[1;32m    719\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n",
      "\u001b[1;32m    720\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n",
      "\u001b[1;32m    721\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n",
      "\u001b[1;32m    722\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n",
      "\u001b[1;32m    723\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 1024, 1024, 3) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2500x1300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a normalizer that can be applied while visualizing masks to have a consistency\n",
    "NORM = mpl.colors.Normalize(vmin=0, vmax=58)\n",
    "\n",
    "# plot masks\n",
    "plt.figure(figsize=(25,13))\n",
    "for i in range(4,7):\n",
    "    plt.subplot(4,6,i)\n",
    "    img = masks[i]\n",
    "    plt.imshow(img, cmap='jet', norm=NORM)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 110)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#functions to resize the images and masks \n",
    "def resize_image(image):\n",
    "    # scale the image\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/255.0\n",
    "    # resize image\n",
    "    image = tf.image.resize(image, (128,128))\n",
    "    return image\n",
    "\n",
    "def resize_mask(mask):\n",
    "    # resize the mask\n",
    "    mask = tf.image.resize(mask, (128,128))\n",
    "    mask = tf.cast(mask, tf.uint8)\n",
    "    return mask    \n",
    "     \n",
    "\n",
    "X = [resize_image(i) for i in images]\n",
    "y = [resize_mask(m) for m in masks]\n",
    "len(X), len(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 128, 128, 3) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[71], line 3\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[39m#visualizing a resized image and respective mask\u001b[39;00m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[39m# plot an image\u001b[39;00m\n",
      "\u001b[0;32m----> 3\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(X[\u001b[39m36\u001b[39;49m])\n",
      "\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mcolorbar()\n",
      "\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/_api/deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n",
      "\u001b[1;32m    454\u001b[0m     warn_deprecated(\n",
      "\u001b[1;32m    455\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[1;32m    458\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/pyplot.py:2652\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   2646\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n",
      "\u001b[1;32m   2647\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n",
      "\u001b[1;32m   2648\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n",
      "\u001b[1;32m   2649\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n",
      "\u001b[1;32m   2650\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n",
      "\u001b[1;32m   2651\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[0;32m-> 2652\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n",
      "\u001b[1;32m   2653\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n",
      "\u001b[1;32m   2654\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n",
      "\u001b[1;32m   2655\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n",
      "\u001b[1;32m   2656\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n",
      "\u001b[1;32m   2657\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n",
      "\u001b[1;32m   2658\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n",
      "\u001b[1;32m   2659\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   2660\u001b[0m     sci(__ret)\n",
      "\u001b[1;32m   2661\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/_api/deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n",
      "\u001b[1;32m    454\u001b[0m     warn_deprecated(\n",
      "\u001b[1;32m    455\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[1;32m    458\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1409\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n",
      "\u001b[1;32m   1410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[1;32m   1411\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m-> 1412\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1414\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32m   1415\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n",
      "\u001b[1;32m   1416\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5481\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   5474\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n",
      "\u001b[1;32m   5475\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap, norm, interpolation,\n",
      "\u001b[1;32m   5476\u001b[0m                       origin, extent, filternorm\u001b[39m=\u001b[39mfilternorm,\n",
      "\u001b[1;32m   5477\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n",
      "\u001b[1;32m   5478\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n",
      "\u001b[1;32m   5479\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m-> 5481\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n",
      "\u001b[1;32m   5482\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n",
      "\u001b[1;32m   5483\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m   5484\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/matplotlib/image.py:715\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n",
      "\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n",
      "\u001b[1;32m    714\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n",
      "\u001b[0;32m--> 715\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    716\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n",
      "\u001b[1;32m    718\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n",
      "\u001b[1;32m    719\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n",
      "\u001b[1;32m    720\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n",
      "\u001b[1;32m    721\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n",
      "\u001b[1;32m    722\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n",
      "\u001b[1;32m    723\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 128, 128, 3) for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAGiCAYAAACGUJO6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbB0lEQVR4nO3df0zd1f3H8RfQcqmx0DrGhbKrrHX+tqWCZVgb53IniQbXPxaZNYURf0xlRnuz2WJbUKulq7Yjs2hj1ekfOqpGjbEEp0xiVJZGWhKdbU2lFWa8tyWu3I4qtNzz/WPfXocFywf50bc8H8nnD84+537OPWH36b2995LgnHMCAMCYxIleAAAAI0HAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACZ5Dtjbb7+t4uJizZo1SwkJCXrllVdOOqe5uVmXXHKJfD6fzj77bD399NMjWCoAAF/zHLCenh7NmzdPdXV1wzp/3759uuaaa3TllVeqra1Nd911l2666Sa9/vrrnhcLAMBxCd/ly3wTEhL08ssva/HixUOes3z5cm3btk0ffvhhfOzXv/61Dh06pMbGxpFeGgAwyU0Z6wu0tLQoGAwOGCsqKtJdd9015Jze3l719vbGf47FYvriiy/0gx/8QAkJCWO1VADAGHDO6fDhw5o1a5YSE0fvrRdjHrBwOCy/3z9gzO/3KxqN6ssvv9S0adNOmFNTU6P77rtvrJcGABhHnZ2d+tGPfjRqtzfmARuJyspKhUKh+M/d3d0688wz1dnZqdTU1AlcGQDAq2g0qkAgoOnTp4/q7Y55wDIzMxWJRAaMRSIRpaamDvrsS5J8Pp98Pt8J46mpqQQMAIwa7X8CGvPPgRUWFqqpqWnA2BtvvKHCwsKxvjQA4HvMc8D+85//qK2tTW1tbZL++zb5trY2dXR0SPrvy3+lpaXx82+99Va1t7fr7rvv1u7du/Xoo4/q+eef17Jly0bnHgAAJiXPAXv//fc1f/58zZ8/X5IUCoU0f/58VVVVSZI+//zzeMwk6cc//rG2bdumN954Q/PmzdOGDRv0xBNPqKioaJTuAgBgMvpOnwMbL9FoVGlpaeru7ubfwADAmLF6DOe7EAEAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYNKIAlZXV6ecnBylpKSooKBA27dv/9bza2trde6552ratGkKBAJatmyZvvrqqxEtGAAAaQQB27p1q0KhkKqrq7Vjxw7NmzdPRUVFOnDgwKDnP/fcc1qxYoWqq6u1a9cuPfnkk9q6davuueee77x4AMDk5TlgGzdu1M0336zy8nJdcMEF2rx5s0477TQ99dRTg57/3nvvaeHChVqyZIlycnJ01VVX6frrrz/pszYAAL6Np4D19fWptbVVwWDw6xtITFQwGFRLS8ugcy677DK1trbGg9Xe3q6GhgZdffXVQ16nt7dX0Wh0wAEAwP+a4uXkrq4u9ff3y+/3Dxj3+/3avXv3oHOWLFmirq4uXX755XLO6dixY7r11lu/9SXEmpoa3XfffV6WBgCYZMb8XYjNzc1au3atHn30Ue3YsUMvvfSStm3bpjVr1gw5p7KyUt3d3fGjs7NzrJcJADDG0zOw9PR0JSUlKRKJDBiPRCLKzMwcdM7q1au1dOlS3XTTTZKkiy++WD09Pbrlllu0cuVKJSae2FCfzyefz+dlaQCAScbTM7Dk5GTl5eWpqakpPhaLxdTU1KTCwsJB5xw5cuSESCUlJUmSnHNe1wsAgCSPz8AkKRQKqaysTPn5+VqwYIFqa2vV09Oj8vJySVJpaamys7NVU1MjSSouLtbGjRs1f/58FRQUaO/evVq9erWKi4vjIQMAwCvPASspKdHBgwdVVVWlcDis3NxcNTY2xt/Y0dHRMeAZ16pVq5SQkKBVq1bps88+0w9/+EMVFxfrwQcfHL17AQCYdBKcgdfxotGo0tLS1N3drdTU1IleDgDAg7F6DOe7EAEAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYNKIAlZXV6ecnBylpKSooKBA27dv/9bzDx06pIqKCmVlZcnn8+mcc85RQ0PDiBYMAIAkTfE6YevWrQqFQtq8ebMKCgpUW1uroqIi7dmzRxkZGSec39fXp1/84hfKyMjQiy++qOzsbH366aeaMWPGaKwfADBJJTjnnJcJBQUFuvTSS7Vp0yZJUiwWUyAQ0B133KEVK1accP7mzZv10EMPaffu3Zo6deqIFhmNRpWWlqbu7m6lpqaO6DYAABNjrB7DPb2E2NfXp9bWVgWDwa9vIDFRwWBQLS0tg8559dVXVVhYqIqKCvn9fl100UVau3at+vv7h7xOb2+votHogAMAgP/lKWBdXV3q7++X3+8fMO73+xUOhwed097erhdffFH9/f1qaGjQ6tWrtWHDBj3wwANDXqempkZpaWnxIxAIeFkmAGASGPN3IcZiMWVkZOjxxx9XXl6eSkpKtHLlSm3evHnIOZWVleru7o4fnZ2dY71MAIAxnt7EkZ6erqSkJEUikQHjkUhEmZmZg87JysrS1KlTlZSUFB87//zzFQ6H1dfXp+Tk5BPm+Hw++Xw+L0sDAEwynp6BJScnKy8vT01NTfGxWCympqYmFRYWDjpn4cKF2rt3r2KxWHzs448/VlZW1qDxAgBgODy/hBgKhbRlyxY988wz2rVrl2677Tb19PSovLxcklRaWqrKysr4+bfddpu++OIL3Xnnnfr444+1bds2rV27VhUVFaN3LwAAk47nz4GVlJTo4MGDqqqqUjgcVm5urhobG+Nv7Ojo6FBi4tddDAQCev3117Vs2TLNnTtX2dnZuvPOO7V8+fLRuxcAgEnH8+fAJgKfAwMAu06Jz4EBAHCqIGAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADApBEFrK6uTjk5OUpJSVFBQYG2b98+rHn19fVKSEjQ4sWLR3JZAADiPAds69atCoVCqq6u1o4dOzRv3jwVFRXpwIED3zpv//79+v3vf69FixaNeLEAABznOWAbN27UzTffrPLycl1wwQXavHmzTjvtND311FNDzunv79cNN9yg++67T7Nnzz7pNXp7exWNRgccAAD8L08B6+vrU2trq4LB4Nc3kJioYDColpaWIefdf//9ysjI0I033jis69TU1CgtLS1+BAIBL8sEAEwCngLW1dWl/v5++f3+AeN+v1/hcHjQOe+8846efPJJbdmyZdjXqaysVHd3d/zo7Oz0skwAwCQwZSxv/PDhw1q6dKm2bNmi9PT0Yc/z+Xzy+XxjuDIAgHWeApaenq6kpCRFIpEB45FIRJmZmSec/8knn2j//v0qLi6Oj8Visf9eeMoU7dmzR3PmzBnJugEAk5ynlxCTk5OVl5enpqam+FgsFlNTU5MKCwtPOP+8887TBx98oLa2tvhx7bXX6sorr1RbWxv/tgUAGDHPLyGGQiGVlZUpPz9fCxYsUG1trXp6elReXi5JKi0tVXZ2tmpqapSSkqKLLrpowPwZM2ZI0gnjAAB44TlgJSUlOnjwoKqqqhQOh5Wbm6vGxsb4Gzs6OjqUmMgXfAAAxlaCc85N9CJOJhqNKi0tTd3d3UpNTZ3o5QAAPBirx3CeKgEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwKQRBayurk45OTlKSUlRQUGBtm/fPuS5W7Zs0aJFizRz5kzNnDlTwWDwW88HAGA4PAds69atCoVCqq6u1o4dOzRv3jwVFRXpwIEDg57f3Nys66+/Xm+99ZZaWloUCAR01VVX6bPPPvvOiwcATF4JzjnnZUJBQYEuvfRSbdq0SZIUi8UUCAR0xx13aMWKFSed39/fr5kzZ2rTpk0qLS0d9Jze3l719vbGf45GowoEAuru7lZqaqqX5QIAJlg0GlVaWtqoP4Z7egbW19en1tZWBYPBr28gMVHBYFAtLS3Duo0jR47o6NGjOuOMM4Y8p6amRmlpafEjEAh4WSYAYBLwFLCuri719/fL7/cPGPf7/QqHw8O6jeXLl2vWrFkDIvhNlZWV6u7ujh+dnZ1elgkAmASmjOfF1q1bp/r6ejU3NyslJWXI83w+n3w+3ziuDABgjaeApaenKykpSZFIZMB4JBJRZmbmt859+OGHtW7dOr355puaO3eu95UCAPA/PL2EmJycrLy8PDU1NcXHYrGYmpqaVFhYOOS89evXa82aNWpsbFR+fv7IVwsAwP/z/BJiKBRSWVmZ8vPztWDBAtXW1qqnp0fl5eWSpNLSUmVnZ6umpkaS9Mc//lFVVVV67rnnlJOTE/+3stNPP12nn376KN4VAMBk4jlgJSUlOnjwoKqqqhQOh5Wbm6vGxsb4Gzs6OjqUmPj1E7vHHntMfX19+tWvfjXgdqqrq3Xvvfd+t9UDACYtz58Dmwhj9RkCAMDYOyU+BwYAwKmCgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTRhSwuro65eTkKCUlRQUFBdq+ffu3nv/CCy/ovPPOU0pKii6++GI1NDSMaLEAABznOWBbt25VKBRSdXW1duzYoXnz5qmoqEgHDhwY9Pz33ntP119/vW688Ubt3LlTixcv1uLFi/Xhhx9+58UDACavBOec8zKhoKBAl156qTZt2iRJisViCgQCuuOOO7RixYoTzi8pKVFPT49ee+21+NhPf/pT5ebmavPmzYNeo7e3V729vfGfu7u7deaZZ6qzs1OpqalelgsAmGDRaFSBQECHDh1SWlra6N2w86C3t9clJSW5l19+ecB4aWmpu/baawedEwgE3J/+9KcBY1VVVW7u3LlDXqe6utpJ4uDg4OD4Hh2ffPKJl+Sc1BR50NXVpf7+fvn9/gHjfr9fu3fvHnROOBwe9PxwODzkdSorKxUKheI/Hzp0SGeddZY6OjpGt97fM8f/K4dnqt+OfTo59mh42KfhOf4q2hlnnDGqt+spYOPF5/PJ5/OdMJ6WlsYvyTCkpqayT8PAPp0cezQ87NPwJCaO7hvfPd1aenq6kpKSFIlEBoxHIhFlZmYOOiczM9PT+QAADIengCUnJysvL09NTU3xsVgspqamJhUWFg46p7CwcMD5kvTGG28MeT4AAMPh+SXEUCiksrIy5efna8GCBaqtrVVPT4/Ky8slSaWlpcrOzlZNTY0k6c4779QVV1yhDRs26JprrlF9fb3ef/99Pf7448O+ps/nU3V19aAvK+Jr7NPwsE8nxx4ND/s0PGO1T57fRi9JmzZt0kMPPaRwOKzc3Fz9+c9/VkFBgSTpZz/7mXJycvT000/Hz3/hhRe0atUq7d+/Xz/5yU+0fv16XX311aN2JwAAk8+IAgYAwETjuxABACYRMACASQQMAGASAQMAmHTKBIw/0TI8XvZpy5YtWrRokWbOnKmZM2cqGAyedF+/D7z+Lh1XX1+vhIQELV68eGwXeIrwuk+HDh1SRUWFsrKy5PP5dM4550yK/9953afa2lqde+65mjZtmgKBgJYtW6avvvpqnFY7Md5++20VFxdr1qxZSkhI0CuvvHLSOc3Nzbrkkkvk8/l09tlnD3jn+rCN6jcrjlB9fb1LTk52Tz31lPvnP//pbr75ZjdjxgwXiUQGPf/dd991SUlJbv369e6jjz5yq1atclOnTnUffPDBOK98fHndpyVLlri6ujq3c+dOt2vXLveb3/zGpaWluX/961/jvPLx43WPjtu3b5/Lzs52ixYtcr/85S/HZ7ETyOs+9fb2uvz8fHf11Ve7d955x+3bt881Nze7tra2cV75+PK6T88++6zz+Xzu2Wefdfv27XOvv/66y8rKcsuWLRvnlY+vhoYGt3LlSvfSSy85SSd84fs3tbe3u9NOO82FQiH30UcfuUceecQlJSW5xsZGT9c9JQK2YMECV1FREf+5v7/fzZo1y9XU1Ax6/nXXXeeuueaaAWMFBQXut7/97Ziuc6J53advOnbsmJs+fbp75plnxmqJE24ke3Ts2DF32WWXuSeeeMKVlZVNioB53afHHnvMzZ492/X19Y3XEk8JXvepoqLC/fznPx8wFgqF3MKFC8d0naeS4QTs7rvvdhdeeOGAsZKSEldUVOTpWhP+EmJfX59aW1sVDAbjY4mJiQoGg2ppaRl0TktLy4DzJamoqGjI878PRrJP33TkyBEdPXp01L8R+lQx0j26//77lZGRoRtvvHE8ljnhRrJPr776qgoLC1VRUSG/36+LLrpIa9euVX9//3gte9yNZJ8uu+wytba2xl9mbG9vV0NDA1/c8A2j9Rg+4d9GP15/osW6kezTNy1fvlyzZs064Rfn+2Ike/TOO+/oySefVFtb2zis8NQwkn1qb2/X3//+d91www1qaGjQ3r17dfvtt+vo0aOqrq4ej2WPu5Hs05IlS9TV1aXLL79czjkdO3ZMt956q+65557xWLIZQz2GR6NRffnll5o2bdqwbmfCn4FhfKxbt0719fV6+eWXlZKSMtHLOSUcPnxYS5cu1ZYtW5Senj7RyzmlxWIxZWRk6PHHH1deXp5KSkq0cuXKIf+q+mTV3NystWvX6tFHH9WOHTv00ksvadu2bVqzZs1EL+17acKfgfEnWoZnJPt03MMPP6x169bpzTff1Ny5c8dymRPK6x598skn2r9/v4qLi+NjsVhMkjRlyhTt2bNHc+bMGdtFT4CR/C5lZWVp6tSpSkpKio+df/75CofD6uvrU3Jy8piueSKMZJ9Wr16tpUuX6qabbpIkXXzxxerp6dEtt9yilStXjvrfw7JqqMfw1NTUYT/7kk6BZ2D8iZbhGck+SdL69eu1Zs0aNTY2Kj8/fzyWOmG87tF5552nDz74QG1tbfHj2muv1ZVXXqm2tjYFAoHxXP64Gcnv0sKFC7V379544CXp448/VlZW1vcyXtLI9unIkSMnROp49B1fOxs3ao/h3t5fMjbq6+udz+dzTz/9tPvoo4/cLbfc4mbMmOHC4bBzzrmlS5e6FStWxM9/99133ZQpU9zDDz/sdu3a5aqrqyfN2+i97NO6detccnKye/HFF93nn38ePw4fPjxRd2HMed2jb5os70L0uk8dHR1u+vTp7ne/+53bs2ePe+2111xGRoZ74IEHJuoujAuv+1RdXe2mT5/u/vrXv7r29nb3t7/9zc2ZM8ddd911E3UXxsXhw4fdzp073c6dO50kt3HjRrdz50736aefOuecW7FihVu6dGn8/ONvo//DH/7gdu3a5erq6uy+jd455x555BF35plnuuTkZLdgwQL3j3/8I/6/XXHFFa6srGzA+c8//7w755xzXHJysrvwwgvdtm3bxnnFE8PLPp111llO0glHdXX1+C98HHn9XfpfkyVgznnfp/fee88VFBQ4n8/nZs+e7R588EF37NixcV71+POyT0ePHnX33nuvmzNnjktJSXGBQMDdfvvt7t///vf4L3wcvfXWW4M+1hzfm7KyMnfFFVecMCc3N9clJye72bNnu7/85S+er8ufUwEAmDTh/wYGAMBIEDAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGDS/wFzTP77mPX4nAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing a resized image and respective mask\n",
    "# plot an image\n",
    "plt.imshow(X[36])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "#plot a mask\n",
    "plt.imshow(y[36], cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1, 128, 128, 3), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(1, 128, 128, 3), dtype=tf.uint8, name=None),\n",
       " TensorSpec(shape=(1, 128, 128, 3), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(1, 128, 128, 3), dtype=tf.uint8, name=None))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split data into 80/20 ratio\n",
    "train_X, val_X,train_y, val_y = train_test_split(X, y, test_size=0.2, \n",
    "                                                      random_state=0\n",
    "                                                     )\n",
    "# develop tf Dataset objects\n",
    "train_X = tf.data.Dataset.from_tensor_slices(train_X)\n",
    "val_X = tf.data.Dataset.from_tensor_slices(val_X)\n",
    "\n",
    "train_y = tf.data.Dataset.from_tensor_slices(train_y)\n",
    "val_y = tf.data.Dataset.from_tensor_slices(val_y)\n",
    "\n",
    "# verify the shapes and data types\n",
    "train_X.element_spec, train_y.element_spec, val_X.element_spec, val_y.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust brightness of image\n",
    "# don't alter in mask\n",
    "def brightness(img, mask):\n",
    "    img = tf.image.adjust_brightness(img, 0.1)\n",
    "    return img, mask\n",
    "\n",
    "# adjust gamma of image\n",
    "# don't alter in mask\n",
    "def gamma(img, mask):\n",
    "    img = tf.image.adjust_gamma(img, 0.1)\n",
    "    return img, mask\n",
    "\n",
    "# adjust hue of image\n",
    "# don't alter in mask\n",
    "def hue(img, mask):\n",
    "    img = tf.image.adjust_hue(img, -0.1)\n",
    "    return img, mask\n",
    "\n",
    "def crop(img, mask):\n",
    "    # crop both image and mask identically\n",
    "    img = tf.image.central_crop(img, 0.7)\n",
    "    # resize after cropping\n",
    "    img = tf.image.resize(img, (128,128))\n",
    "    mask = tf.image.central_crop(mask, 0.7)\n",
    "    # resize afer cropping\n",
    "    mask = tf.image.resize(mask, (128,128))\n",
    "    # cast to integers as they are class numbers\n",
    "    mask = tf.cast(mask, tf.uint8)\n",
    "    return img, mask\n",
    "# flip both image and mask identically\n",
    "def flip_hori(img, mask):\n",
    "    img = tf.image.flip_left_right(img)\n",
    "    mask = tf.image.flip_left_right(mask)\n",
    "    return img, mask\n",
    "\n",
    "# flip both image and mask identically\n",
    "def flip_vert(img, mask):\n",
    "    img = tf.image.flip_up_down(img)\n",
    "    mask = tf.image.flip_up_down(mask)\n",
    "    return img, mask\n",
    "\n",
    "# rotate both image and mask identically\n",
    "def rotate(img, mask):\n",
    "    img = tf.image.rot90(img)\n",
    "    mask = tf.image.rot90(mask)\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip images and masks\n",
    "train = tf.data.Dataset.zip((train_X, train_y))\n",
    "val = tf.data.Dataset.zip((val_X, val_y))\n",
    "\n",
    "# perform augmentation on train data only\n",
    "\n",
    "a = train.map(brightness)\n",
    "b = train.map(gamma)\n",
    "c = train.map(hue)\n",
    "d = train.map(crop)\n",
    "e = train.map(flip_hori)\n",
    "f = train.map(flip_vert)\n",
    "g = train.map(rotate)\n",
    "\n",
    "# concatenate every new augmented sets\n",
    "train = train.concatenate(a)\n",
    "train = train.concatenate(b)\n",
    "train = train.concatenate(c)\n",
    "train = train.concatenate(d)\n",
    "train = train.concatenate(e)\n",
    "train = train.concatenate(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the batch size\n",
    "BATCH = 64\n",
    "\n",
    "AT = tf.data.AUTOTUNE\n",
    "#buffersize\n",
    "BUFFER = 1000\n",
    "\n",
    "STEPS_PER_EPOCH = 800//BATCH\n",
    "VALIDATION_STEPS = 200//BATCH\n",
    "\n",
    "train = train.cache().shuffle(BUFFER).batch(BATCH).repeat()\n",
    "train = train.prefetch(buffer_size=AT)\n",
    "val = val.batch(BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From hereon, I added my code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 13:19:16.855917: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x296fec720 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-01 13:19:16.856061: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-08-01 13:19:16.963984: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Conv._jit_compiled_convolution_op at 0x29f834cc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Conv._jit_compiled_convolution_op at 0x29f836a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at nvidia/mit-b0 were not used when initializing TFSegformerForSemanticSegmentation: ['classifier']\n",
      "- This IS expected if you are initializing TFSegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFSegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFSegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "# this part was take from: https://keras.io/examples/vision/segformer/\n",
    "\n",
    "from transformers import TFSegformerForSemanticSegmentation\n",
    "\n",
    "model_checkpoint = \"nvidia/mit-b0\"\n",
    "id2label = {0: \"no-damage\", 1: \"minor-damage\", 2: \"major-damage\", 3: \"destroyed\"}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "model = TFSegformerForSemanticSegmentation.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# this part was take from: https://keras.io/examples/vision/segformer/\n",
    "lr = 0.00006\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_segformer_for_semantic_segmentation_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " segformer (TFSegformerMain  multiple                  3319392   \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " decode_head (TFSegformerDe  multiple                  396292    \n",
      " codeHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3715684 (14.17 MB)\n",
      "Trainable params: 3715172 (14.17 MB)\n",
      "Non-trainable params: 512 (2.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(1, 128, 128, 3), dtype=tf.float32, name=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_X.element_spec)\n",
    "print(train_y.element_spec)\n",
    "print(train.element_spec)\n",
    "print(\"---\"*15)\n",
    "print(val_X.element_spec)\n",
    "print(val_y.element_spec)\n",
    "print(val.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part was take from: https://keras.io/examples/vision/segformer/\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.math.argmax(pred_mask, axis=1)\n",
    "    pred_mask = tf.expand_dims(pred_mask, -1)\n",
    "    return pred_mask[0]\n",
    "\n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for sample in dataset.take(num):\n",
    "            images, masks = sample[\"pixel_values\"], sample[\"labels\"]\n",
    "            masks = tf.expand_dims(masks, -1)\n",
    "            pred_masks = model.predict(images).logits\n",
    "            images = tf.transpose(images, (0, 2, 3, 1))\n",
    "            display([images[0], masks[0], create_mask(pred_masks)])\n",
    "    else:\n",
    "        display(\n",
    "            [\n",
    "                sample_image,\n",
    "                sample_mask,\n",
    "                create_mask(model.predict(tf.expand_dims(sample_image, 0))),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions(self.dataset)\n",
    "        print(\"\\nSample Prediction after epoch {}\\n\".format(epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1642, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file0pmdg5ai.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file18x776y4.py\", line 41, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).segformer, (ag__.ld(pixel_values),), dict(output_attentions=ag__.ld(output_attentions), output_hidden_states=True, return_dict=ag__.ld(return_dict)), fscope)\n    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file0pmdg5ai.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file_rp2oc8c.py\", line 15, in tf__call\n        encoder_outputs = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(pixel_values),), dict(output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 103, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.converted_call(ag__.ld(zip), (ag__.ld(self).embeddings, ag__.ld(self).block, ag__.ld(self).layer_norms), None, fscope),), None, fscope), None, loop_body_1, get_state_4, set_state_4, ('all_hidden_states', 'all_self_attentions', 'hidden_states'), {'iterate_names': '(idx, x)'})\n    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 27, in loop_body_1\n        hidden_states, height, width = ag__.converted_call(ag__.ld(embedding_layer), (ag__.ld(hidden_states),), None, fscope)\n    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_filexiuwx6bx.py\", line 11, in tf__call\n        embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_segformer_for_semantic_segmentation_4' (type TFSegformerForSemanticSegmentation).\n    \n    in user code:\n    \n        File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 792, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 819, in call  *\n            outputs = self.segformer(\n        File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file0pmdg5ai.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file_rp2oc8c.py\", line 15, in tf__call\n            encoder_outputs = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(pixel_values),), dict(output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n        File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 103, in tf__call\n            ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.converted_call(ag__.ld(zip), (ag__.ld(self).embeddings, ag__.ld(self).block, ag__.ld(self).layer_norms), None, fscope),), None, fscope), None, loop_body_1, get_state_4, set_state_4, ('all_hidden_states', 'all_self_attentions', 'hidden_states'), {'iterate_names': '(idx, x)'})\n        File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 27, in loop_body_1\n            hidden_states, height, width = ag__.converted_call(ag__.ld(embedding_layer), (ag__.ld(hidden_states),), None, fscope)\n        File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_filexiuwx6bx.py\", line 11, in tf__call\n            embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'segformer' (type TFSegformerMainLayer).\n        \n        in user code:\n        \n            File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 792, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 484, in call  *\n                encoder_outputs = self.encoder(\n            File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 103, in tf__call\n                ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.converted_call(ag__.ld(zip), (ag__.ld(self).embeddings, ag__.ld(self).block, ag__.ld(self).layer_norms), None, fscope),), None, fscope), None, loop_body_1, get_state_4, set_state_4, ('all_hidden_states', 'all_self_attentions', 'hidden_states'), {'iterate_names': '(idx, x)'})\n            File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 27, in loop_body_1\n                hidden_states, height, width = ag__.converted_call(ag__.ld(embedding_layer), (ag__.ld(hidden_states),), None, fscope)\n            File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_filexiuwx6bx.py\", line 11, in tf__call\n                embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'encoder' (type TFSegformerEncoder).\n            \n            in user code:\n            \n                File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 419, in call  *\n                    hidden_states, height, width = embedding_layer(hidden_states)\n                File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_filexiuwx6bx.py\", line 11, in tf__call\n                    embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n            \n                ValueError: Exception encountered when calling layer 'patch_embeddings.0' (type TFSegformerOverlapPatchEmbeddings).\n                \n                in user code:\n                \n                    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 92, in call  *\n                        embeddings = self.proj(self.padding(pixel_values))\n                    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n                        raise ValueError(\n                \n                    ValueError: Input 0 of layer \"proj\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (1, 134, 9, 128)\n                \n                \n                Call arguments received by layer 'patch_embeddings.0' (type TFSegformerOverlapPatchEmbeddings):\n                  • pixel_values=tf.Tensor(shape=(1, 128, 3, 128), dtype=float32)\n            \n            \n            Call arguments received by layer 'encoder' (type TFSegformerEncoder):\n              • pixel_values=tf.Tensor(shape=(1, 128, 3, 128), dtype=float32)\n              • output_attentions=False\n              • output_hidden_states=True\n              • return_dict=True\n              • training=True\n        \n        \n        Call arguments received by layer 'segformer' (type TFSegformerMainLayer):\n          • pixel_values=tf.Tensor(shape=(1, 128, 128, 3), dtype=float32)\n          • output_attentions=False\n          • output_hidden_states=True\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tf_segformer_for_semantic_segmentation_4' (type TFSegformerForSemanticSegmentation):\n      • pixel_values={'pixel_values': 'tf.Tensor(shape=(1, 128, 128, 3), dtype=float32)', 'labels': 'tf.Tensor(shape=(1, 128, 128, 3), dtype=uint8)'}\n      • labels=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[81], line 9\u001b[0m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[39m# Zip the input features and target data together for the validation dataset\u001b[39;00m\n",
      "\u001b[1;32m      7\u001b[0m val_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip((val_X, val_y))\n",
      "\u001b[0;32m----> 9\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n",
      "\u001b[1;32m     10\u001b[0m     train_data,\n",
      "\u001b[1;32m     11\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_data,\n",
      "\u001b[1;32m     12\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[DisplayCallback(val_data)],  \u001b[39m# Replace 'val_data' with 'val_data' if needed\u001b[39;49;00m\n",
      "\u001b[1;32m     13\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n",
      "\u001b[1;32m     14\u001b[0m )\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n",
      "\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n",
      "\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[0;32m/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_filehxe8r1sn.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1642\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n",
      "\u001b[1;32m   1640\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m   1641\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1642\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m   1643\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_using_dummy_loss:\n",
      "\u001b[1;32m   1644\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_loss(y_pred\u001b[39m.\u001b[39mloss, y_pred\u001b[39m.\u001b[39mloss, sample_weight, regularization_losses\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses)\n",
      "\n",
      "File \u001b[0;32m/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file0pmdg5ai.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n",
      "\u001b[1;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file18x776y4.py:41\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m     39\u001b[0m return_dict \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mif_exp(ag__\u001b[39m.\u001b[39mld(return_dict) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m: ag__\u001b[39m.\u001b[39mld(return_dict), \u001b[39mlambda\u001b[39;00m: ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict, \u001b[39m'\u001b[39m\u001b[39mreturn_dict is not None\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m     40\u001b[0m output_hidden_states \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mif_exp(ag__\u001b[39m.\u001b[39mld(output_hidden_states) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m: ag__\u001b[39m.\u001b[39mld(output_hidden_states), \u001b[39mlambda\u001b[39;00m: ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states, \u001b[39m'\u001b[39m\u001b[39moutput_hidden_states is not None\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;32m---> 41\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49msegformer, (ag__\u001b[39m.\u001b[39;49mld(pixel_values),), \u001b[39mdict\u001b[39;49m(output_attentions\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_dict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(return_dict)), fscope)\n",
      "\u001b[1;32m     42\u001b[0m encoder_hidden_states \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mif_exp(ag__\u001b[39m.\u001b[39mld(return_dict), \u001b[39mlambda\u001b[39;00m: ag__\u001b[39m.\u001b[39mld(outputs)\u001b[39m.\u001b[39mhidden_states, \u001b[39mlambda\u001b[39;00m: ag__\u001b[39m.\u001b[39mld(outputs)[\u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m     43\u001b[0m logits \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdecode_head, (ag__\u001b[39m.\u001b[39mld(encoder_hidden_states),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\n",
      "File \u001b[0;32m/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file0pmdg5ai.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n",
      "\u001b[1;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file_rp2oc8c.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n",
      "\u001b[1;32m     13\u001b[0m return_dict \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mif_exp(ag__\u001b[39m.\u001b[39mld(return_dict) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m: ag__\u001b[39m.\u001b[39mld(return_dict), \u001b[39mlambda\u001b[39;00m: ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict, \u001b[39m'\u001b[39m\u001b[39mreturn_dict is not None\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m     14\u001b[0m pixel_values \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mtranspose, (ag__\u001b[39m.\u001b[39mld(pixel_values),), \u001b[39mdict\u001b[39m(perm\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)), fscope)\n",
      "\u001b[0;32m---> 15\u001b[0m encoder_outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mencoder, (ag__\u001b[39m.\u001b[39;49mld(pixel_values),), \u001b[39mdict\u001b[39;49m(output_attentions\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(output_hidden_states), return_dict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(return_dict), training\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(training)), fscope)\n",
      "\u001b[1;32m     16\u001b[0m sequence_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(encoder_outputs)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m     17\u001b[0m sequence_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mtranspose, (ag__\u001b[39m.\u001b[39mld(sequence_output),), \u001b[39mdict\u001b[39m(perm\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m]), fscope)\n",
      "\n",
      "File \u001b[0;32m/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py:103\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n",
      "\u001b[1;32m    101\u001b[0m num_channels \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mnum_channels\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m    102\u001b[0m block_layer \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mblock_layer\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;32m--> 103\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39menumerate\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mzip\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49membeddings, ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mblock, ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlayer_norms), \u001b[39mNone\u001b[39;49;00m, fscope),), \u001b[39mNone\u001b[39;49;00m, fscope), \u001b[39mNone\u001b[39;49;00m, loop_body_1, get_state_4, set_state_4, (\u001b[39m'\u001b[39;49m\u001b[39mall_hidden_states\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mall_self_attentions\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mhidden_states\u001b[39;49m\u001b[39m'\u001b[39;49m), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39m(idx, x)\u001b[39;49m\u001b[39m'\u001b[39;49m})\n",
      "\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_5\u001b[39m():\n",
      "\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m (do_return, retval_)\n",
      "\n",
      "File \u001b[0;32m/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py:27\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body_1\u001b[0;34m(itr_1)\u001b[0m\n",
      "\u001b[1;32m     25\u001b[0m idx, x \u001b[39m=\u001b[39m itr_1\n",
      "\u001b[1;32m     26\u001b[0m embedding_layer, block_layer, norm_layer \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(x)\n",
      "\u001b[0;32m---> 27\u001b[0m hidden_states, height, width \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(embedding_layer), (ag__\u001b[39m.\u001b[39;49mld(hidden_states),), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_1\u001b[39m():\n",
      "\u001b[1;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m (all_self_attentions, hidden_states)\n",
      "\n",
      "File \u001b[0;32m/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_filexiuwx6bx.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, pixel_values)\u001b[0m\n",
      "\u001b[1;32m      9\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32m     10\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n",
      "\u001b[0;32m---> 11\u001b[0m embeddings \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mproj, (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mpadding, (ag__\u001b[39m.\u001b[39;49mld(pixel_values),), \u001b[39mNone\u001b[39;49;00m, fscope),), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "\u001b[1;32m     12\u001b[0m height \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(shape_list), (ag__\u001b[39m.\u001b[39mld(embeddings),), \u001b[39mNone\u001b[39;00m, fscope)[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m     13\u001b[0m width \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(shape_list), (ag__\u001b[39m.\u001b[39mld(embeddings),), \u001b[39mNone\u001b[39;00m, fscope)[\u001b[39m2\u001b[39m]\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n",
      "\n",
      "    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1642, in train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file0pmdg5ai.py\", line 40, in tf__run_call_with_unpacked_inputs\n",
      "        raise\n",
      "    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file18x776y4.py\", line 41, in tf__call\n",
      "        outputs = ag__.converted_call(ag__.ld(self).segformer, (ag__.ld(pixel_values),), dict(output_attentions=ag__.ld(output_attentions), output_hidden_states=True, return_dict=ag__.ld(return_dict)), fscope)\n",
      "    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file0pmdg5ai.py\", line 40, in tf__run_call_with_unpacked_inputs\n",
      "        raise\n",
      "    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file_rp2oc8c.py\", line 15, in tf__call\n",
      "        encoder_outputs = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(pixel_values),), dict(output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n",
      "    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 103, in tf__call\n",
      "        ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.converted_call(ag__.ld(zip), (ag__.ld(self).embeddings, ag__.ld(self).block, ag__.ld(self).layer_norms), None, fscope),), None, fscope), None, loop_body_1, get_state_4, set_state_4, ('all_hidden_states', 'all_self_attentions', 'hidden_states'), {'iterate_names': '(idx, x)'})\n",
      "    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 27, in loop_body_1\n",
      "        hidden_states, height, width = ag__.converted_call(ag__.ld(embedding_layer), (ag__.ld(hidden_states),), None, fscope)\n",
      "    File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_filexiuwx6bx.py\", line 11, in tf__call\n",
      "        embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n",
      "\n",
      "    ValueError: Exception encountered when calling layer 'tf_segformer_for_semantic_segmentation_4' (type TFSegformerForSemanticSegmentation).\n",
      "    \n",
      "    in user code:\n",
      "    \n",
      "        File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 792, in run_call_with_unpacked_inputs  *\n",
      "            return func(self, **unpacked_inputs)\n",
      "        File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 819, in call  *\n",
      "            outputs = self.segformer(\n",
      "        File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n",
      "            raise e.with_traceback(filtered_tb) from None\n",
      "        File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file0pmdg5ai.py\", line 40, in tf__run_call_with_unpacked_inputs\n",
      "            raise\n",
      "        File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_file_rp2oc8c.py\", line 15, in tf__call\n",
      "            encoder_outputs = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(pixel_values),), dict(output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n",
      "        File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 103, in tf__call\n",
      "            ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.converted_call(ag__.ld(zip), (ag__.ld(self).embeddings, ag__.ld(self).block, ag__.ld(self).layer_norms), None, fscope),), None, fscope), None, loop_body_1, get_state_4, set_state_4, ('all_hidden_states', 'all_self_attentions', 'hidden_states'), {'iterate_names': '(idx, x)'})\n",
      "        File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 27, in loop_body_1\n",
      "            hidden_states, height, width = ag__.converted_call(ag__.ld(embedding_layer), (ag__.ld(hidden_states),), None, fscope)\n",
      "        File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_filexiuwx6bx.py\", line 11, in tf__call\n",
      "            embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n",
      "    \n",
      "        ValueError: Exception encountered when calling layer 'segformer' (type TFSegformerMainLayer).\n",
      "        \n",
      "        in user code:\n",
      "        \n",
      "            File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 792, in run_call_with_unpacked_inputs  *\n",
      "                return func(self, **unpacked_inputs)\n",
      "            File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 484, in call  *\n",
      "                encoder_outputs = self.encoder(\n",
      "            File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n",
      "                raise e.with_traceback(filtered_tb) from None\n",
      "            File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 103, in tf__call\n",
      "                ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.converted_call(ag__.ld(zip), (ag__.ld(self).embeddings, ag__.ld(self).block, ag__.ld(self).layer_norms), None, fscope),), None, fscope), None, loop_body_1, get_state_4, set_state_4, ('all_hidden_states', 'all_self_attentions', 'hidden_states'), {'iterate_names': '(idx, x)'})\n",
      "            File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_fileh84u71fl.py\", line 27, in loop_body_1\n",
      "                hidden_states, height, width = ag__.converted_call(ag__.ld(embedding_layer), (ag__.ld(hidden_states),), None, fscope)\n",
      "            File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_filexiuwx6bx.py\", line 11, in tf__call\n",
      "                embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n",
      "        \n",
      "            ValueError: Exception encountered when calling layer 'encoder' (type TFSegformerEncoder).\n",
      "            \n",
      "            in user code:\n",
      "            \n",
      "                File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 419, in call  *\n",
      "                    hidden_states, height, width = embedding_layer(hidden_states)\n",
      "                File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n",
      "                    raise e.with_traceback(filtered_tb) from None\n",
      "                File \"/var/folders/hl/hfkbp3_54j7g9499gz2266zw0000gn/T/__autograph_generated_filexiuwx6bx.py\", line 11, in tf__call\n",
      "                    embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n",
      "            \n",
      "                ValueError: Exception encountered when calling layer 'patch_embeddings.0' (type TFSegformerOverlapPatchEmbeddings).\n",
      "                \n",
      "                in user code:\n",
      "                \n",
      "                    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 92, in call  *\n",
      "                        embeddings = self.proj(self.padding(pixel_values))\n",
      "                    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n",
      "                        raise e.with_traceback(filtered_tb) from None\n",
      "                    File \"/Users/sevincjakab/neuefische_bootcamp/20230717-NewRepo-Capstone-Building_Damage/Capstone_Building_Damage/.venv/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n",
      "                        raise ValueError(\n",
      "                \n",
      "                    ValueError: Input 0 of layer \"proj\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (1, 134, 9, 128)\n",
      "                \n",
      "                \n",
      "                Call arguments received by layer 'patch_embeddings.0' (type TFSegformerOverlapPatchEmbeddings):\n",
      "                  • pixel_values=tf.Tensor(shape=(1, 128, 3, 128), dtype=float32)\n",
      "            \n",
      "            \n",
      "            Call arguments received by layer 'encoder' (type TFSegformerEncoder):\n",
      "              • pixel_values=tf.Tensor(shape=(1, 128, 3, 128), dtype=float32)\n",
      "              • output_attentions=False\n",
      "              • output_hidden_states=True\n",
      "              • return_dict=True\n",
      "              • training=True\n",
      "        \n",
      "        \n",
      "        Call arguments received by layer 'segformer' (type TFSegformerMainLayer):\n",
      "          • pixel_values=tf.Tensor(shape=(1, 128, 128, 3), dtype=float32)\n",
      "          • output_attentions=False\n",
      "          • output_hidden_states=True\n",
      "          • return_dict=True\n",
      "          • training=True\n",
      "    \n",
      "    \n",
      "    Call arguments received by layer 'tf_segformer_for_semantic_segmentation_4' (type TFSegformerForSemanticSegmentation):\n",
      "      • pixel_values={'pixel_values': 'tf.Tensor(shape=(1, 128, 128, 3), dtype=float32)', 'labels': 'tf.Tensor(shape=(1, 128, 128, 3), dtype=uint8)'}\n",
      "      • labels=None\n",
      "      • output_attentions=None\n",
      "      • output_hidden_states=None\n",
      "      • return_dict=None\n"
     ]
    }
   ],
   "source": [
    "# ??not sure if I need to use train and val or train_X and val_X\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    callbacks=[DisplayCallback(val)],\n",
    "    epochs=2,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
