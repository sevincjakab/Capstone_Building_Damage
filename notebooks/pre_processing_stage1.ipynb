{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6776bd-1ac4-4306-ae0d-7772e9395e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "Collecting absl-py (from tensorflow_datasets)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting array-record (from tensorflow_datasets)\n",
      "  Downloading array_record-0.4.0-py310-none-any.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click (from tensorflow_datasets)\n",
      "  Using cached click-8.1.6-py3-none-any.whl (97 kB)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-macosx_11_0_arm64.whl (110 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting etils[enp,epath]>=0.9.0 (from tensorflow_datasets)\n",
      "  Using cached etils-1.4.1-py3-none-any.whl (135 kB)\n",
      "Requirement already satisfied: numpy in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow_datasets) (1.24.3)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting protobuf>=3.20 (from tensorflow_datasets)\n",
      "  Using cached protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "Requirement already satisfied: psutil in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow_datasets) (5.9.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Using cached tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
      "Collecting termcolor (from tensorflow_datasets)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tqdm in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow_datasets) (4.65.0)\n",
      "Collecting wrapt (from tensorflow_datasets)\n",
      "  Using cached wrapt-1.15.0-cp311-cp311-macosx_11_0_arm64.whl (36 kB)\n",
      "Collecting importlib_resources (from etils[enp,epath]>=0.9.0->tensorflow_datasets)\n",
      "  Using cached importlib_resources-6.0.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: typing_extensions in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.6.2)\n",
      "Collecting zipp (from etils[enp,epath]>=0.9.0->tensorflow_datasets)\n",
      "  Using cached zipp-3.16.2-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.5.7)\n",
      "Requirement already satisfied: six in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21484 sha256=1c20fc8787baaa898b676b6159d3afb9c25f563121a6a2c43de6e53ae54ada49\n",
      "  Stored in directory: /Users/gmeneses/Library/Caches/pip/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, zipp, wrapt, toml, termcolor, protobuf, promise, importlib_resources, etils, click, absl-py, googleapis-common-protos, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "Successfully installed absl-py-1.4.0 array-record-0.4.0 click-8.1.6 dm-tree-0.1.8 etils-1.4.1 googleapis-common-protos-1.60.0 importlib_resources-6.0.0 promise-2.3 protobuf-4.23.4 tensorflow-metadata-1.13.1 tensorflow_datasets-4.9.2 termcolor-2.3.0 toml-0.10.2 wrapt-1.15.0 zipp-3.16.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers -q\n",
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d82f44b-9dfb-4c21-8088-227a353cf819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.0-cp311-cp311-macosx_12_0_arm64.whl (1.9 kB)\n",
      "Collecting tensorflow-macos==2.13.0 (from tensorflow)\n",
      "  Using cached tensorflow_macos-2.13.0-cp311-cp311-macosx_12_0_arm64.whl (189.3 MB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.4.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached h5py-3.9.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl (20.6 MB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached grpcio-1.56.2-cp311-cp311-macosx_10_10_universal2.whl (8.9 MB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached wheel-0.41.0-py3-none-any.whl (64 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.2)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wheel, werkzeug, urllib3, typing-extensions, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, oauthlib, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, rsa, pyasn1-modules, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-macos, tensorflow\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.2\n",
      "    Uninstalling urllib3-2.0.2:\n",
      "      Successfully uninstalled urllib3-2.0.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.6.2\n",
      "    Uninstalling typing_extensions-4.6.2:\n",
      "      Successfully uninstalled typing_extensions-4.6.2\n",
      "Successfully installed astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.2 h5py-3.9.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-macos-2.13.0 typing-extensions-4.5.0 urllib3-1.26.16 werkzeug-2.3.6 wheel-0.41.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791cdc2e-a348-452b-b6cb-fb4735fdbc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e93f7dfd-e5e0-4514-a0aa-d298c23bf49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load(\"oxford_iiit_pet:3.*.*\",with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58d1b13a-a92d-4c81-b2c5-c382cecb965f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <_PrefetchDataset element_spec={'file_name': TensorSpec(shape=(), dtype=tf.string, name=None), 'image': TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'segmentation_mask': TensorSpec(shape=(None, None, 1), dtype=tf.uint8, name=None), 'species': TensorSpec(shape=(), dtype=tf.int64, name=None)}>,\n",
       " 'test': <_PrefetchDataset element_spec={'file_name': TensorSpec(shape=(), dtype=tf.string, name=None), 'image': TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'segmentation_mask': TensorSpec(shape=(None, None, 1), dtype=tf.uint8, name=None), 'species': TensorSpec(shape=(), dtype=tf.int64, name=None)}>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ea95b50-7e70-4a71-bc32-978a8143bec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='oxford_iiit_pet',\n",
       "    full_name='oxford_iiit_pet/3.2.0',\n",
       "    description=\"\"\"\n",
       "    The Oxford-IIIT pet dataset is a 37 category pet image dataset with roughly 200\n",
       "    images for each class. The images have large variations in scale, pose and\n",
       "    lighting. All images have an associated ground truth annotation of breed.\n",
       "    \"\"\",\n",
       "    homepage='http://www.robots.ox.ac.uk/~vgg/data/pets/',\n",
       "    data_path='/Users/gmeneses/tensorflow_datasets/oxford_iiit_pet/3.2.0',\n",
       "    file_format=tfrecord,\n",
       "    download_size=773.52 MiB,\n",
       "    dataset_size=774.69 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'file_name': Text(shape=(), dtype=string),\n",
       "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=37),\n",
       "        'segmentation_mask': Image(shape=(None, None, 1), dtype=uint8),\n",
       "        'species': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=3669, num_shards=4>,\n",
       "        'train': <SplitInfo num_examples=3680, num_shards=4>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{parkhi12a,\n",
       "      author       = \"Parkhi, O. M. and Vedaldi, A. and Zisserman, A. and Jawahar, C.~V.\",\n",
       "      title        = \"Cats and Dogs\",\n",
       "      booktitle    = \"IEEE Conference on Computer Vision and Pattern Recognition\",\n",
       "      year         = \"2012\",\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50f9a1a6-e74a-4920-b998-bc2d461910e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f038bdde-abfb-40fd-965b-cc62dea3b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "mean = tf.constant([0.485, 0.456, 0.406])\n",
    "std = tf.constant([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def normalize(input_image, input_mask):\n",
    "    input_image = tf.image.convert_image_dtype(input_image, tf.float32)\n",
    "    input_image = (input_image - mean) / tf.maximum(std, backend.epsilon())\n",
    "    input_mask -= 1\n",
    "    return input_image, input_mask\n",
    "\n",
    "\n",
    "def load_image(datapoint):\n",
    "    input_image = tf.image.resize(datapoint[\"image\"], (image_size, image_size))\n",
    "    input_mask = tf.image.resize(\n",
    "        datapoint[\"segmentation_mask\"],\n",
    "        (image_size, image_size),\n",
    "        method=\"bilinear\",\n",
    "    )\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    input_image = tf.transpose(input_image, (2, 0, 1))\n",
    "    return {\"pixel_values\": input_image, \"labels\": tf.squeeze(input_mask)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05f5bc63-085b-42a4-bc3a-4bf3ed3ebc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (500, 500, 3)\n",
      "Image data type: <dtype: 'uint8'>\n",
      "Mask shape: (500, 500, 1)\n",
      "Mask data type: <dtype: 'uint8'>\n"
     ]
    }
   ],
   "source": [
    "# checking one image ans mask of the tensowflow dataset\n",
    "sample = next(iter(dataset[\"train\"]))\n",
    "image, mask = sample[\"image\"], sample[\"segmentation_mask\"]\n",
    "\n",
    "# Inspect the shape and data type of the image and mask\n",
    "print(\"Image shape:\", image.shape)  # E.g., (height, width, channels)\n",
    "print(\"Image data type:\", image.dtype)  # E.g., float32\n",
    "\n",
    "print(\"Mask shape:\", mask.shape)  # E.g., (height, width, 1)\n",
    "print(\"Mask data type:\", mask.dtype)  # E.g., int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71dfcdf8-2d48-42d7-aac9-f64d14588d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mask_to_single_channel(mask_3_channels):\n",
    "    # Assuming mask_3_channels has shape (height, width, 3)\n",
    "    height, width, _ = mask_3_channels.shape\n",
    "\n",
    "    # Create an empty array with shape (height, width, 1) for the single-channel mask\n",
    "    single_channel_mask = np.zeros((height, width, 1), dtype=np.uint8)\n",
    "\n",
    "    # Define the colors representing each category (RGB values)\n",
    "    category_colors = {\n",
    "        (0, 0, 0): 0,        # Class 0 - Black (no building) or un-classified\n",
    "        (255, 255, 255): 1,  # Class 1 - White (no-damage)\n",
    "        (255, 0, 0): 2,      # Class 2 - Red (minor damage)\n",
    "        (0, 255, 0): 3,      # Class 3 - Green (major damage)\n",
    "        (0, 0, 255): 4,      # Class 4 - Blue (destroyed)\n",
    "    }\n",
    "    # Loop through each pixel and assign the corresponding category to the single-channel mask\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            pixel_color = tuple(mask_3_channels[y, x])\n",
    "            category = category_colors.get(pixel_color, -1)  # -1 for unknown category\n",
    "            single_channel_mask[y, x] = category\n",
    "\n",
    "    return single_channel_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a70924b-3f1e-4faf-a059-068045e686c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def read_png_image(file_path):\n",
    "    image = Image.open(file_path)\n",
    "    image_array = np.array(image)\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a60d4e0b-1d67-4f5e-a219-aa9ff5513c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating image and mask arrays: 100%|████████████████████████████████████████████████████| 1/1 [00:34<00:00, 34.54s/disaster]\n"
     ]
    }
   ],
   "source": [
    "# need to change this it's too slow. Here I'm just working with a subset of guatemala-volcano\n",
    "from os import path, walk, makedirs\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_example=\"/Users/gmeneses/DScourse/00_capstone/xView2_baseline_fork/xBD_last_subset_test_mask\"\n",
    "disasters = next(walk(path_example))[1]\n",
    "\n",
    "image_arrays = []\n",
    "mask_arrays = []\n",
    "for disaster in tqdm(disasters, desc='Creating image and mask arrays', unit='disaster'):\n",
    "    # Create the full path to the images, labels, and mask output directories\n",
    "    image_dir = path.join(path_example, disaster, 'images')\n",
    "    mask_dir = path.join(path_example, disaster, 'masks')\n",
    "\n",
    "    if not path.isdir(image_dir):\n",
    "        print(\n",
    "            \"Error, could not find image files in {}.\\n\\n\"\n",
    "            .format(image_dir),\n",
    "            file=stderr)\n",
    "        exit(2)\n",
    "\n",
    "    if not path.isdir(mask_dir):\n",
    "        print(\n",
    "            \"Error, could not find labels in {}.\\n\\n\"\n",
    "            .format(json_dir),\n",
    "            file=stderr)\n",
    "        exit(3)\n",
    "    \n",
    "        \n",
    "    # running through masks because it can be that there are no masks for certain images ()\n",
    "    # in this case masks have the same name than images\n",
    "    masks_list = [j for j in next(walk(mask_dir))[2] if '_post' in j]\n",
    "    for im in masks_list:\n",
    "        img_pre_name = path.splitext(im.replace('_post', '_pre'))[0] + '.png'\n",
    "        img_post_name = im\n",
    "        mask_name = im\n",
    "        # path to images and mask\n",
    "        img_pre = path.join(image_dir,img_pre_name)\n",
    "        img_post = path.join(image_dir,img_post_name)\n",
    "        mask = path.join(mask_dir,mask_name)\n",
    "        \n",
    "        array_pre = read_png_image(img_pre)\n",
    "        array_post = read_png_image(img_post)\n",
    "        array_mask_3d = read_png_image(mask)\n",
    "        \n",
    "        # converting mask to depth 1\n",
    "        array_mask = convert_mask_to_single_channel(array_mask_3d)\n",
    "        # creating a final image array (1024x1024x6) \n",
    "        array_image = np.concatenate((array_pre, array_post), axis=-1)\n",
    "        # adding to lists in array format\n",
    "        image_arrays.append(array_image)\n",
    "        mask_arrays.append(array_mask)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c3e9a7b-54b3-4326-a057-4a2fa0a3cd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1024, 1024, 6)\n",
      "(12, 1024, 1024, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to numpy arrays\n",
    "images = np.stack(image_arrays)\n",
    "masks = np.stack(mask_arrays)\n",
    "\n",
    "print(images.shape)\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b8d9a0d-367d-43b6-96ae-2d6b4d7f2c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the arrays in npz numpy format \n",
    "np.savez('input_model.npz', images=images, masks=masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a712e74-b263-4417-a2ee-653ea2908cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to recover images and mask arrays:\n",
    "loaded_arrays = np.load('input_model.npz')\n",
    "images = loaded_arrays['images']\n",
    "masks = loaded_arrays['masks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8088372f-742d-4eaa-816d-233073062c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# to see the pixel values of only mask number 5\n",
    "print(masks[5, :, :, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "452945ad-3aba-499e-9cdf-7694ea9deb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 4]\n"
     ]
    }
   ],
   "source": [
    "# see the values that fill the mask number 5 \n",
    "print(np.unique(masks[5, :, :, 0])) # [0 1 4] --> only buildings with damage levels 1 and 4 in this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c6b2e-0dd6-42f7-afef-f1b474f4bd22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
