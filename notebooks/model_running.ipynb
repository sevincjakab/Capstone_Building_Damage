{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I followed the below blogpage until the implementation of the model: \n",
    "\n",
    "I had to add one additional dimension to the images for 'batch' that SegFormer model needs. That is why matplotlib functions will give an error (just skip for now). The ultimate error is for the last cell. This error occurs because the proj layer expects the input tensor to have a shape with the last dimension (axis -1) equal to 3, but the input shape received by the proj layer is (1, 134, 9, 128).\n",
    "\n",
    "The issue seems to be related to the data preprocessing or input configuration for your model. To fix this, you should check the following:\n",
    "\n",
    "Verify that the images in your dataset have the correct shape (height, width, channels). In this case, the channels should be 3, as it represents the RGB color channels.\n",
    "Check the data preprocessing pipeline for your images. Ensure that the images are being loaded and resized correctly to the expected input shape of the model.\n",
    "Verify the configuration of the proj layer or any other layer in your segformer model that might expect a specific input shape. Make sure the input shape is compatible with the model architecture.\n",
    "Ensure that the input images are correctly passed to the model during training and validation.\n",
    "\n",
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2023/04/deep-learning-for-image-segmentation-with-tensorflow/\n",
    "\n",
    "Some cells used: \n",
    "https://keras.io/examples/vision/segformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is a previous version of the one4all and stepwise notebooks.\n",
    "### It uses images with 3 channels and png masks with 1 channel (it converts pngs with 3 channels to 1 channel) and is using post image and classification masks. The rest is similar to the one4all and stepwise notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# A list to collect paths of 1000 images\n",
    "image_path = []\n",
    "for root, dirs, files in os.walk('/Users/gmeneses/DScourse/00_capstone/xView2_baseline_fork/xBD_last_subset_test_mask/guatemala-volcano/images'):\n",
    "    # Iterate over 1000 images\n",
    "    for file in files:\n",
    "        # Check if the file has a PNG extension\n",
    "        if file.lower().endswith('.png') and '_post' in file:\n",
    "            # Create path\n",
    "            path = os.path.join(root, file)\n",
    "            # Add path to list\n",
    "            image_path.append(path)\n",
    "            \n",
    "print(len(image_path))\n",
    "\n",
    "# A list to collect paths of 1000 masks\n",
    "mask_path = []\n",
    "for root, dirs, files in os.walk('/Users/gmeneses/DScourse/00_capstone/xView2_baseline_fork/xBD_last_subset_test_mask/guatemala-volcano/masks'):\n",
    "    # Iterate over 1000 masks\n",
    "    for file in files:\n",
    "        # Check if the file has a PNG extension\n",
    "        if file.lower().endswith('.png'):\n",
    "            # Obtain the path\n",
    "            path = os.path.join(root, file)\n",
    "            # Add path to the list\n",
    "            mask_path.append(path)\n",
    "print(len(mask_path))\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mask_to_single_channel(mask_3_channels):\n",
    "    # Assuming mask_3_channels has shape (height, width, 3)\n",
    "    height, width, _ = mask_3_channels.shape\n",
    "\n",
    "    # Create an empty array with shape (height, width, 1) for the single-channel mask\n",
    "    single_channel_mask = np.zeros((height, width, 1), dtype=np.uint8)\n",
    "\n",
    "    # Define the colors representing each category (RGB values)\n",
    "    category_colors = {\n",
    "        (0, 0, 0): 0,        # Class 0 - Black (no building) or un-classified\n",
    "        (255, 255, 255): 1,  # Class 1 - White (no-damage)\n",
    "        (255, 0, 0): 2,      # Class 2 - Red (minor damage)\n",
    "        (0, 255, 0): 3,      # Class 3 - Green (major damage)\n",
    "        (0, 0, 255): 4,      # Class 4 - Blue (destroyed)\n",
    "    }\n",
    "    # Loop through each pixel and assign the corresponding category to the single-channel mask\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            pixel_color = tuple(mask_3_channels[y, x])\n",
    "            category = category_colors.get(pixel_color, -1)  # -1 for unknown category\n",
    "            single_channel_mask[y, x] = category\n",
    "\n",
    "    return single_channel_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 37.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:35<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# here we are converting png images and masks to arrays\n",
    "from PIL import Image\n",
    "# create a list to store images\n",
    "images = []\n",
    "# iterate over 1000 image paths\n",
    "for path in tqdm(image_path):\n",
    "    # read file\n",
    "    file = tf.io.read_file(path)\n",
    "    # decode png file into a tensor\n",
    "    image = tf.image.decode_png(file, channels=3, dtype=tf.uint8)\n",
    "\n",
    "    # #adding 4th dimension for batch size \n",
    "    # image = tf.expand_dims(image, axis=0)\n",
    "    # append to the list\n",
    "    images.append(image)\n",
    "\n",
    "\n",
    "# create a list to store masks and converts to single channel\n",
    "masks = []\n",
    "# iterate over 1000 mask paths\n",
    "for path in tqdm(mask_path):\n",
    "    # read the file\n",
    "    #file = tf.io.read_file(path)\n",
    "    file = Image.open(path)\n",
    "    mask_3_channels = np.array(file)\n",
    "    # decode png file into a tensor\n",
    "    #mask_3_channels = tf.image.decode_png(file, channels=3, dtype=tf.uint8)\n",
    "    mask = convert_mask_to_single_channel(mask_3_channels)\n",
    "    # #adding 4th dimension for batch size \n",
    "    # mask = tf.expand_dims(mask, axis=0)\n",
    "    # append mask to the list\n",
    "    masks.append(tf.convert_to_tensor(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n",
      "(1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the first image\n",
    "for i,im in enumerate(images):\n",
    "    print(images[i].shape)\n",
    "#print(type(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(25,13))\n",
    "\n",
    "# # Iterate over the images in the range 4-6\n",
    "# for i in range(4,7):\n",
    "#     # Create a subplot for each image\n",
    "#     plt.subplot(4,6,i)\n",
    "#     # Get the i-th image from the list\n",
    "#     img = images[i]\n",
    "#     # Show the image with a colorbar\n",
    "#     plt.imshow(img)\n",
    "#     plt.colorbar()\n",
    "#     # Turn off the axis labels\n",
    "#     plt.axis('off')\n",
    "\n",
    "# # Display the figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a normalizer that can be applied while visualizing masks to have a consistency\n",
    "# NORM = mpl.colors.Normalize(vmin=0, vmax=58)\n",
    "\n",
    "# # plot masks\n",
    "# plt.figure(figsize=(25,13))\n",
    "# for i in range(4,7):\n",
    "#     plt.subplot(4,6,i)\n",
    "#     img = masks[i]\n",
    "#     plt.imshow(img, cmap='jet', norm=NORM)\n",
    "#     plt.colorbar()\n",
    "#     plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #functions to resize the images and masks \n",
    "# def resize_image(image):\n",
    "#     # scale the image\n",
    "#     image = tf.cast(image, tf.float32)\n",
    "#     image = image/255.0\n",
    "#     # resize image\n",
    "#     image = tf.image.resize(image, (128,128))\n",
    "#     return image\n",
    "\n",
    "# def resize_mask(mask):\n",
    "#     # resize the mask\n",
    "#     mask = tf.image.resize(mask, (128,128))\n",
    "#     mask = tf.cast(mask, tf.uint8)\n",
    "#     return mask    \n",
    "\n",
    "\n",
    "\n",
    "#X = [resize_image(i) for i in images]\n",
    "#y = [resize_mask(m) for m in masks]\n",
    "\n",
    "X = [i for i in images]\n",
    "y = [m for m in masks]\n",
    "len(X), len(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024, 3)\n",
      "(1024, 1024, 1)\n"
     ]
    }
   ],
   "source": [
    "print (X[0].shape)\n",
    "print (y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #visualizing a resized image and respective mask\n",
    "# # plot an image\n",
    "# plt.imshow(X[11])\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# #plot a mask\n",
    "# plt.imshow(y[11], cmap='jet')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 80/20 ratio\n",
    "train_X, val_X,train_y, val_y = train_test_split(X, y, test_size=0.2, \n",
    "                                                      random_state=0\n",
    "                                                     )\n",
    "# develop tf Dataset objects\n",
    "#train_X = tf.data.Dataset.from_tensor_slices(train_X)\n",
    "#val_X = tf.data.Dataset.from_tensor_slices(val_X)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X,train_y))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_X,val_y))\n",
    "\n",
    "#train_y = tf.data.Dataset.from_tensor_slices(train_y)\n",
    "#val_y = tf.data.Dataset.from_tensor_slices(val_y)\n",
    "#print(len(train_X) ,len(val_X),len(train_y),len(val_y))\n",
    "# verify the shapes and data types\n",
    "#train_X.element_spec, train_y.element_spec, val_X.element_spec, val_y.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (1024, 1024, 3)\n",
      "Image data type: <dtype: 'uint8'>\n",
      "Mask shape: (1024, 1024, 1)\n",
      "Mask data type: <dtype: 'uint8'>\n"
     ]
    }
   ],
   "source": [
    "# sample = next(iter(train_dataset))\n",
    "# image, mask = sample[0], sample[1]\n",
    "\n",
    "# # Inspect the shape and data type of the image and mask\n",
    "# print(\"Image shape:\", image.shape)  # E.g., (height, width, channels)\n",
    "# print(\"Image data type:\", image.dtype)  # E.g., float32\n",
    "\n",
    "# print(\"Mask shape:\", mask.shape)  # E.g., (height, width, 1)\n",
    "# print(\"Mask data type:\", mask.dtype)  # E.g., int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=(TensorSpec(shape=(1024, 1024, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(1024, 1024, 1), dtype=tf.uint8, name=None))>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Augmentation functions ARE NOT implemented in this notebook\n",
    "\n",
    "# # adjust brightness of image\n",
    "# # don't alter in mask\n",
    "# def brightness(img, mask):\n",
    "#     img = tf.image.adjust_brightness(img, 0.1)\n",
    "#     return img, mask\n",
    "\n",
    "# # adjust gamma of image\n",
    "# # don't alter in mask\n",
    "# def gamma(img, mask):\n",
    "#     img = tf.image.adjust_gamma(img, 0.1)\n",
    "#     return img, mask\n",
    "\n",
    "# # adjust hue of image\n",
    "# # don't alter in mask\n",
    "# def hue(img, mask):\n",
    "#     img = tf.image.adjust_hue(img, -0.1)\n",
    "#     return img, mask\n",
    "\n",
    "# def crop(img, mask):\n",
    "#     # crop both image and mask identically\n",
    "#     img = tf.image.central_crop(img, 0.7)\n",
    "#     # resize after cropping\n",
    "#     img = tf.image.resize(img, (128,128))\n",
    "#     mask = tf.image.central_crop(mask, 0.7)\n",
    "#     # resize afer cropping\n",
    "#     mask = tf.image.resize(mask, (128,128))\n",
    "#     # cast to integers as they are class numbers\n",
    "#     mask = tf.cast(mask, tf.uint8)\n",
    "#     return img, mask\n",
    "# # flip both image and mask identically\n",
    "# def flip_hori(img, mask):\n",
    "#     img = tf.image.flip_left_right(img)\n",
    "#     mask = tf.image.flip_left_right(mask)\n",
    "#     return img, mask\n",
    "\n",
    "# # flip both image and mask identically\n",
    "# def flip_vert(img, mask):\n",
    "#     img = tf.image.flip_up_down(img)\n",
    "#     mask = tf.image.flip_up_down(mask)\n",
    "#     return img, mask\n",
    "\n",
    "# # rotate both image and mask identically\n",
    "# def rotate(img, mask):\n",
    "#     img = tf.image.rot90(img)\n",
    "#     mask = tf.image.rot90(mask)\n",
    "#     return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # zip images and masks\n",
    "# #train = tf.data.Dataset.zip((train_X, train_y))\n",
    "# #val = tf.data.Dataset.zip((val_X, val_y))\n",
    "\n",
    "# # perform augmentation on train data only\n",
    "\n",
    "# a = train_dataset.map(brightness)\n",
    "# b = train_dataset.map(gamma)\n",
    "# c = train_dataset.map(hue)\n",
    "# d = train_dataset.map(crop)\n",
    "# e = train_dataset.map(flip_hori)\n",
    "# f = train_dataset.map(flip_vert)\n",
    "# g = train_dataset.map(rotate)\n",
    "\n",
    "# # concatenate every new augmented sets\n",
    "# train_dataset = train_dataset.concatenate(a)\n",
    "# train_dataset = train_dataset.concatenate(b)\n",
    "# train_dataset = train_dataset.concatenate(c)\n",
    "# train_dataset = train_dataset.concatenate(d)\n",
    "# train_dataset = train_dataset.concatenate(e)\n",
    "# train_dataset = train_dataset.concatenate(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fn(image, mask):\n",
    "    # Assign names to the elements in the dataset\n",
    "    return {\"image\": image, \"segmentation_mask\": mask}\n",
    "named_dataset_train = train_dataset.map(map_fn)\n",
    "named_dataset_val = val_dataset.map(map_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (1024, 1024, 3)\n",
      "Image data type: <dtype: 'uint8'>\n",
      "Mask shape: (1024, 1024, 1)\n",
      "Mask data type: <dtype: 'uint8'>\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(named_dataset_train))\n",
    "image, mask = sample[\"image\"], sample[\"segmentation_mask\"]\n",
    "\n",
    "# Inspect the shape and data type of the image and mask\n",
    "print(\"Image shape:\", image.shape)  # E.g., (height, width, channels)\n",
    "print(\"Image data type:\", image.dtype)  # E.g., float32\n",
    "\n",
    "print(\"Mask shape:\", mask.shape)  # E.g., (height, width, 1)\n",
    "print(\"Mask data type:\", mask.dtype)  # E.g., int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "\n",
    "image_size = 512\n",
    "mean = tf.constant([0.485, 0.456, 0.406])\n",
    "std = tf.constant([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def normalize(input_image, input_mask):\n",
    "    input_image = tf.image.convert_image_dtype(input_image, tf.float32)\n",
    "    input_image = (input_image - mean) / tf.maximum(std, backend.epsilon())\n",
    "    #input_mask -= 1\n",
    "    return input_image, input_mask\n",
    "\n",
    "\n",
    "def load_image(datapoint):\n",
    "    input_image = tf.image.resize(datapoint[\"image\"], (image_size, image_size))\n",
    "    input_mask = tf.image.resize(\n",
    "        datapoint[\"segmentation_mask\"],\n",
    "        (image_size, image_size),\n",
    "        method=\"bilinear\",\n",
    "    )\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    input_image = tf.transpose(input_image, (2, 0, 1))\n",
    "    return {\"pixel_values\": input_image, \"labels\": tf.squeeze(input_mask)}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the batch size\n",
    "# BATCH = 4\n",
    "\n",
    "# AT = tf.data.AUTOTUNE\n",
    "# #buffersize\n",
    "# BUFFER = 6\n",
    "\n",
    "# STEPS_PER_EPOCH = 9//BATCH # total number of training samples / batch\n",
    "# VALIDATION_STEPS = 3//BATCH # total number of validation samples / batch\n",
    "\n",
    "\n",
    "# #caches the data in memory to speed up data loading - shuffles - batches\n",
    "# train = (named_dataset_train\n",
    "#          .cache()\n",
    "#          .shuffle(BUFFER)\n",
    "#          .map(load_image, num_parallel_calls=AT)\n",
    "#          .batch(BATCH)\n",
    "#          .prefetch(buffer_size=AT)\n",
    "#          )\n",
    "\n",
    "# val = (named_dataset_val\n",
    "#        .map(load_image, num_parallel_calls=AT)\n",
    "#        .batch(BATCH)\n",
    "#        .prefetch(buffer_size=AT)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = tf.data.AUTOTUNE\n",
    "batch_size = 4\n",
    "\n",
    "train = (\n",
    "    named_dataset_train\n",
    "    .cache()\n",
    "    .shuffle(batch_size * 10)\n",
    "    .map(load_image, num_parallel_calls=auto)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(auto)\n",
    ")\n",
    "val = (\n",
    "    named_dataset_val\n",
    "    .map(load_image, num_parallel_calls=auto)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(auto)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec={'pixel_values': TensorSpec(shape=(None, 3, 512, 512), dtype=tf.float32, name=None), 'labels': TensorSpec(shape=(None, 512, 512), dtype=tf.float32, name=None)}>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From hereon, I added my code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-02 11:18:00.890987: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28d91a7c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-02 11:18:00.891128: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-08-02 11:18:01.013470: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Conv._jit_compiled_convolution_op at 0x28da4dbc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Conv._jit_compiled_convolution_op at 0x28da4f920> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at nvidia/mit-b0 were not used when initializing TFSegformerForSemanticSegmentation: ['classifier']\n",
      "- This IS expected if you are initializing TFSegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFSegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFSegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "# this part was take from: https://keras.io/examples/vision/segformer/\n",
    "\n",
    "from transformers import TFSegformerForSemanticSegmentation\n",
    "\n",
    "model_checkpoint = \"nvidia/mit-b0\"\n",
    "id2label = {0: \"background\", 1: \"no-damage\", 2: \"minor-damage\", 3: \"major-damage\", 4: \"destroyed\"}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "model = TFSegformerForSemanticSegmentation.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# this part was take from: https://keras.io/examples/vision/segformer/\n",
    "# lr = 0.00006\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# model.compile(optimizer=optimizer, loss=loss_function)\n",
    "#\n",
    "\n",
    "lr = 0.00006\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_segformer_for_semantic_segmentation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " segformer (TFSegformerMain  multiple                  3319392   \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " decode_head (TFSegformerDe  multiple                  396549    \n",
      " codeHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3715941 (14.18 MB)\n",
      "Trainable params: 3715429 (14.17 MB)\n",
      "Non-trainable params: 512 (2.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "        plt.axis(\"off\")\n",
    "    # Create the folder if it doesn't exist\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for samples in val.take(2):\n",
    "    \n",
    "    sample_image, sample_mask = samples[\"pixel_values\"][0], samples[\"labels\"][0]\n",
    "    sample_image = tf.transpose(sample_image, (1, 2, 0))\n",
    "    sample_mask = tf.expand_dims(sample_mask, -1)\n",
    "    display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part was take from: https://keras.io/examples/vision/segformer/\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.math.argmax(pred_mask, axis=1)\n",
    "    pred_mask = tf.expand_dims(pred_mask, -1)\n",
    "    return pred_mask[0]\n",
    "\n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for sample in dataset.take(num):\n",
    "            images, masks = sample[\"pixel_values\"], sample[\"labels\"]\n",
    "            masks = tf.expand_dims(masks, -1)\n",
    "            pred_masks = model.predict(images).logits\n",
    "            images = tf.transpose(images, (0, 2, 3, 1))\n",
    "            display([images[0], masks[0], create_mask(pred_masks)])\n",
    "    else:\n",
    "        display(\n",
    "            [\n",
    "                sample_image,\n",
    "                sample_mask,\n",
    "                create_mask(model.predict(tf.expand_dims(sample_image, 0))),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions(self.dataset)\n",
    "        print(\"\\nSample Prediction after epoch {}\\n\".format(epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# ??not sure if I need to use train and val or train_X and val_X\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    callbacks=[DisplayCallback(val)],\n",
    "    epochs=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(512, 512, 3), dtype=float32, numpy=\n",
       " array([[[355.9607 , 420.95535, 333.75113],\n",
       "         [329.75983, 394.16962, 307.08447],\n",
       "         [326.4847 , 391.93747, 302.64   ],\n",
       "         ...,\n",
       "         [208.58078, 367.3839 , 175.97334],\n",
       "         [193.29694, 342.83035, 161.5289 ],\n",
       "         [202.03056, 358.45535, 164.86223]],\n",
       " \n",
       "        [[361.41922, 432.11606, 339.30667],\n",
       "         [357.0524 , 428.76785, 334.86224],\n",
       "         [354.869  , 428.76785, 331.5289 ],\n",
       "         ...,\n",
       "         [199.84715, 347.29462, 168.19557],\n",
       "         [212.94759, 377.42856, 179.30669],\n",
       "         [207.48907, 370.73212, 172.64001]],\n",
       " \n",
       "        [[371.24454, 447.74106, 348.1956 ],\n",
       "         [382.16156, 461.1339 , 358.1956 ],\n",
       "         [382.16156, 463.36606, 358.1956 ],\n",
       "         ...,\n",
       "         [208.58078, 359.5714 , 173.75113],\n",
       "         [212.94759, 377.42856, 177.08446],\n",
       "         [217.3144 , 389.70535, 179.30669]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[194.38864, 263.58926, 147.08446],\n",
       "         [238.05676, 318.27676, 183.75113],\n",
       "         [245.69868, 324.9732 , 192.64001],\n",
       "         ...,\n",
       "         [222.77292, 298.1875 , 184.86223],\n",
       "         [196.57205, 266.9375 , 159.30667],\n",
       "         [205.30568, 273.6339 , 164.86223]],\n",
       " \n",
       "        [[218.40611, 302.65176, 168.19557],\n",
       "         [157.27074, 220.06248, 121.52889],\n",
       "         [255.52402, 340.5982 , 199.30669],\n",
       "         ...,\n",
       "         [241.33188, 322.74106, 204.86223],\n",
       "         [199.84715, 272.51785, 165.97334],\n",
       "         [194.38864, 264.70535, 159.30667]],\n",
       " \n",
       "        [[203.12227, 290.375  , 157.08446],\n",
       "         [204.21397, 284.79462, 154.86223],\n",
       "         [172.55458, 236.80356, 130.41779],\n",
       "         ...,\n",
       "         [233.68996, 312.6964 , 201.5289 ],\n",
       "         [235.87335, 320.5089 , 200.41779],\n",
       "         [202.03056, 275.86606, 167.08446]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(512, 512, 1), dtype=float32, numpy=\n",
       " array([[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       " \n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       " \n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       " \n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       " \n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(128, 128, 1), dtype=int64, numpy=\n",
       " array([[[3],\n",
       "         [3],\n",
       "         [3],\n",
       "         ...,\n",
       "         [4],\n",
       "         [3],\n",
       "         [3]],\n",
       " \n",
       "        [[3],\n",
       "         [3],\n",
       "         [3],\n",
       "         ...,\n",
       "         [3],\n",
       "         [3],\n",
       "         [3]],\n",
       " \n",
       "        [[3],\n",
       "         [3],\n",
       "         [3],\n",
       "         ...,\n",
       "         [3],\n",
       "         [3],\n",
       "         [3]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0],\n",
       "         [3],\n",
       "         [3],\n",
       "         ...,\n",
       "         [1],\n",
       "         [3],\n",
       "         [4]],\n",
       " \n",
       "        [[0],\n",
       "         [3],\n",
       "         [3],\n",
       "         ...,\n",
       "         [4],\n",
       "         [4],\n",
       "         [1]],\n",
       " \n",
       "        [[4],\n",
       "         [3],\n",
       "         [3],\n",
       "         ...,\n",
       "         [1],\n",
       "         [3],\n",
       "         [4]]])>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install ipython\n",
    "from IPython.display import clear_output\n",
    "show_predictions(val, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
