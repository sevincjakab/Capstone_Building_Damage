{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook has the first version of the splitting code, only based on randomly selecting files up to a user defined maximum size (in GB) in the variable \"max_size\". \n",
    "\n",
    "#### source_directory and target_directory are variables that need to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "def get_dataset_size(dataset_dir):\n",
    "    total_size = 0\n",
    "    for root, dirs, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "\n",
    "    # Convert total_size to a more human-readable format (e.g., MB, GB)\n",
    "    total_size_mb = total_size / (1024 * 1024)\n",
    "    total_size_gb = total_size / (1024 * 1024 * 1024)\n",
    "\n",
    "    return total_size, total_size_mb, total_size_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(source_dir, source_folder, target_dir, target_folder, batch_size, max_size):\n",
    "    \n",
    "    total_ids = []\n",
    "    total_size_gb = 0\n",
    "    while total_size_gb <=  max_size:\n",
    "        #list of name files in images and labels directories for the folder specified in \"source_folder\" variable\n",
    "        tif_files = [file for file in os.listdir(source_dir+\"/\"+source_folder+\"/images\") if file.endswith('.tif')]\n",
    "        json_files = [file for file in os.listdir(source_dir+\"/\"+source_folder+\"/labels\") if file.endswith('.json')]\n",
    "        \n",
    "        # collecting id number for all the files in \"source_folder\"\n",
    "        ids = list(set([x.rsplit(\"_\")[1] for x in tif_files]))\n",
    "        \n",
    "        # Randomly select a subset of ids\n",
    "        selected_ids = random.sample(ids, batch_size)\n",
    "        \n",
    "        #getting list of files, given certain id number\n",
    "        \n",
    "        # collecting files in images/hold\n",
    "        tif_files_selected = []\n",
    "        for id in selected_ids:\n",
    "            if id not in total_ids:\n",
    "                tif_files_selected.extend(glob.glob(source_dir+\"/\"+source_folder+\"/images/*\"+id+\"*.tif\"))\n",
    "        #collecting files in labels/hold\n",
    "        json_files_selected = []\n",
    "        for id in selected_ids:\n",
    "            if id not in total_ids:\n",
    "                json_files_selected.extend(glob.glob(source_dir+\"/\"+source_folder+\"/labels/*\"+id+\"*.json\"))        \n",
    "        \n",
    "        # save ids to not repeat\n",
    "        #print(\"ids in this iteration: \",total_ids)\n",
    "        total_ids.extend(selected_ids)\n",
    "        \n",
    "        # Copy the selected files to the target directory (target_folder/images)\n",
    "        for path in tif_files_selected:\n",
    "            file = path.rsplit(\"/\")[-1]\n",
    "            source_path = os.path.join(source_dir, source_folder,\"images\",file)\n",
    "            target_path = os.path.join(target_dir, target_folder,\"images\", file)\n",
    "            #print(source_path)\n",
    "            #print(target_path)\n",
    "            shutil.copyfile(source_path, target_path)\n",
    "\n",
    "        # Copy the selected files to the target directory (target_folder/labels)\n",
    "        for path in json_files_selected:\n",
    "            file = path.rsplit(\"/\")[-1]\n",
    "            source_path = os.path.join(source_dir, source_folder,\"labels\",file)\n",
    "            target_path = os.path.join(target_dir, target_folder,\"labels\", file)\n",
    "            #print(source_path)\n",
    "            #print(target_path)\n",
    "            shutil.copyfile(source_path, target_path)\n",
    "        total_size, total_size_mb, total_size_gb = get_dataset_size(os.path.join(target_dir, target_folder))   \n",
    "        print(\"current target directory size (GB): \",total_size_gb)\n",
    "        #print(\"the final size (GB) of the target directory \",target_dir,\" is: \",total_size_gb)\n",
    "    print(\"Max. size reached!: \",total_size_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the following, change source_directory and target_directory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_dirs = [\"hold\",\"test\",\"tier1\"] # directories in the source directory from which I want to extract data. tier3 has another dataset with different disasters\n",
    "\n",
    "# Set the paths and parameters for creating the subset \n",
    "source_directory = '/Volumes/Elements/data_buidings/geotiffs'  # Path to your large dataset directory\n",
    "target_directory = '/Users/gmeneses/DScourse/00_capstone/Capstone_Building_Damage/data/subset'  # Path to the directory where the subset will be created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is divided between training, test and holdout in the proportions 0.6 , 0.2, 0.2 respectively.\n",
    "\n",
    "The resulting data set has 8.71 GB\n",
    "\n",
    "See below variable \"max_size\" for each folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TRAINING set\n",
    "\n",
    "source_folder = \"tier1\"\n",
    "target_folder = \"training\"\n",
    "#images\n",
    "train_images_dir = os.path.join(target_directory, target_folder,\"images\")\n",
    "print(train_images_dir)\n",
    "#labels\n",
    "train_labels_dir = os.path.join(target_directory, target_folder,\"labels\")\n",
    "# create directories\n",
    "os.makedirs(train_images_dir)\n",
    "os.makedirs(train_labels_dir)\n",
    "\n",
    "# set parameters training set\n",
    "batch_size=5       # Number of images that are randomly selected in each iteration per folder in \"sub_dirs\" variable\n",
    "max_size = 4.8 #in Gb\n",
    "\n",
    "# Call the function to create the subset TRAINING (tier1 is training in original dataset)\n",
    "create_subset(source_directory, source_folder, target_directory, target_folder, batch_size, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TEST set\n",
    "\n",
    "source_folder = \"test\"\n",
    "target_folder = \"test\"\n",
    "#images\n",
    "train_images_dir = os.path.join(target_directory, target_folder,\"images\")\n",
    "print(train_images_dir)\n",
    "#labels\n",
    "train_labels_dir = os.path.join(target_directory, target_folder,\"labels\")\n",
    "# create directories\n",
    "os.makedirs(train_images_dir)\n",
    "os.makedirs(train_labels_dir)\n",
    "\n",
    "# set parameters training set\n",
    "batch_size=5       # Number of images that are randomly selected in each iteration \n",
    "max_size = 1.6 #in Gb\n",
    "\n",
    "# Call the function to create the subset TRAINING (tier1 is training in original dataset)\n",
    "create_subset(source_directory, source_folder, target_directory, target_folder, batch_size, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create HOLDOUT set\n",
    "\n",
    "source_folder = \"hold\"\n",
    "target_folder = \"holdout\"\n",
    "#images\n",
    "train_images_dir = os.path.join(target_directory, target_folder,\"images\")\n",
    "print(train_images_dir)\n",
    "#labels\n",
    "train_labels_dir = os.path.join(target_directory, target_folder,\"labels\")\n",
    "# create directories\n",
    "os.makedirs(train_images_dir)\n",
    "os.makedirs(train_labels_dir)\n",
    "\n",
    "# set parameters training set\n",
    "batch_size=5       # Number of images that are randomly selected in each iteration \n",
    "max_size = 1.6 #in Gb\n",
    "\n",
    "# Call the function to create the subset TRAINING (tier1 is training in original dataset)\n",
    "create_subset(source_directory, source_folder, target_directory, target_folder, batch_size, max_size)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
