{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os import path, walk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_path):\n",
    "    \"\"\"\n",
    "    :param json_path: path to load json from\n",
    "    :returns: a python dictionary of json features\n",
    "    \"\"\"\n",
    "    annotations = json.load(open(json_path))\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disaster_info(feature):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    " \n",
    "    try:\n",
    "        disaster = feature['metadata']['disaster']\n",
    "        disaster_type = feature['metadata']['disaster_type']\n",
    "    except:\n",
    "        disaster = 'unclassified'\n",
    "        disaster_type = 'unclassified'\n",
    "        print(\"no disaster information in \"+feature['metadata']['img_name'])\n",
    "         \n",
    "\n",
    "    return disaster,disaster_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is using labels,only with the propose of showcasing --> we want to identify which type of disaster and its name.\n",
    "# in the case of real new images, it will not be possible to get that information, and only the images directory is needed \n",
    "def create_image_tensors(images_path, json_path=None):\n",
    "    \n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "        \n",
    "    \"\"\"\n",
    "    arr_img_pre_list = []\n",
    "    arr_img_post_list = []\n",
    "\n",
    "    # Check if json_path is a valid directory\n",
    "    if json_path and os.path.isdir(json_path):\n",
    "        #arr_img_pre_list = []\n",
    "        #arr_img_post_list = []\n",
    "        id_pre_list = []\n",
    "        id_post_list = []\n",
    "        dis_list = []\n",
    "        dis_type_list = []\n",
    "\n",
    "        # For each feature in the json we will create a separate mask\n",
    "        # Getting all files in the directory provided for jsons\n",
    "        jsons_pre = [j for j in next(walk(json_path))[2] if '_pre' in j]\n",
    "        # After removing non-json items in dir (if any)\n",
    "        for j in [j for j in jsons_pre if j.endswith('json')]:\n",
    "            # Our chips start off in life as PNGs\n",
    "            chip_image_id_pre = path.splitext(j)[0] + '.png'\n",
    "            chip_image_id_post = path.splitext(j)[0].replace('_pre', '_post') + '.png'\n",
    "\n",
    "            id_pre = path.splitext(j)[0]\n",
    "            id_post = path.splitext(j)[0].replace('_pre', '_post')\n",
    "\n",
    "            # Loading the per chip json pre-disaster\n",
    "            j_full_path_pre = path.join(json_path, j)\n",
    "            chip_json_pre = read_json(j_full_path_pre)\n",
    "            \n",
    "            #getting the name of the post-json\n",
    "            j_post = j.replace('_pre', '_post')\n",
    "            #print(j_post)\n",
    "            \n",
    "            # Loading the per chip json post-disaster\n",
    "            j_full_path_post = path.join(json_path, j_post)\n",
    "            chip_json_post = read_json(j_full_path_post)\n",
    "\n",
    "            # Getting the full chip path, and loading the size dimensions (same for post)\n",
    "            chip_file_pre = path.join(images_path, chip_image_id_pre)\n",
    "            chip_file_post = path.join(images_path, chip_image_id_post)\n",
    "\n",
    "            #chip_img_size = get_dimensions(chip_file_pre)\n",
    "            #chip_size = (chip_img_size[0],chip_img_size[0],1)\n",
    "            # Reading in the polygons from the json file\n",
    "            #polys_pre = get_feature_info(chip_json_pre)\n",
    "            #polys_post = get_feature_damage(chip_json_post)\n",
    "            # creating tensors from images\n",
    "            img_pre = tf.io.read_file(chip_file_pre)\n",
    "            img_post = tf.io.read_file(chip_file_post)\n",
    "            array_pre = tf.image.decode_png(img_pre, channels=3, dtype=tf.uint8)\n",
    "            array_post = tf.image.decode_png(img_post, channels=3, dtype=tf.uint8)\n",
    "            \n",
    "            dis,dis_type = get_disaster_info(chip_json_pre)\n",
    "            \n",
    "            arr_img_pre_list.append(array_pre)\n",
    "            arr_img_post_list.append(array_post)\n",
    "            #creating id and id_disaster lists to identify the arrays\n",
    "            id_pre_list.append(id_pre)\n",
    "            id_post_list.append(id_post)\n",
    "            dis_list.append(dis)\n",
    "            dis_type_list.append(dis_type)\n",
    "        return arr_img_pre_list, arr_img_post_list, id_pre_list, id_post_list, dis_list, dis_type_list\n",
    "       \n",
    "    else:\n",
    "        #arr_img_pre_list = []\n",
    "        #arr_img_post_list = []\n",
    "       \n",
    "\n",
    "        # For each feature in the json we will create a separate mask\n",
    "        # Getting all files in the directory provided for jsons\n",
    "        images_pre = [j for j in next(walk(images_path))[2] if '_pre' in j]\n",
    "        # After removing non-json items in dir (if any)\n",
    "        for j in [j for j in images_pre if j.endswith('png')]:\n",
    "            # Our chips start off in life as PNGs\n",
    "            chip_image_id_pre = j\n",
    "            chip_image_id_post = j.replace('_pre', '_post')\n",
    "\n",
    "            id_pre = path.splitext(j)[0]\n",
    "            id_post = path.splitext(j)[0].replace('_pre', '_post')\n",
    "\n",
    "            # Getting the full chip path, and loading the size dimensions (same for post)\n",
    "            chip_file_pre = path.join(images_path, chip_image_id_pre)\n",
    "            chip_file_post = path.join(images_path, chip_image_id_post)\n",
    "\n",
    "            img_pre = tf.io.read_file(chip_file_pre)\n",
    "            img_post = tf.io.read_file(chip_file_post)\n",
    "            array_pre = tf.image.decode_png(img_pre, channels=3, dtype=tf.uint8)\n",
    "            array_post = tf.image.decode_png(img_post, channels=3, dtype=tf.uint8)\n",
    "            \n",
    "            \n",
    "            \n",
    "            arr_img_pre_list.append(array_pre)\n",
    "            arr_img_post_list.append(array_post)\n",
    "            \n",
    "        return arr_img_pre_list, arr_img_post_list\n",
    "        \n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without label info\n",
    "# img_pre,img_post = create_image_tensors(\"/Users/gmeneses/DScourse/00_capstone/xView2_baseline_fork/data4inference/images\")\n",
    "\n",
    "# images_pre = np.stack(img_pre)\n",
    "# images_post = np.stack(img_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with label info = give json_path and images_path (only to get the disaster label, showcase proposes)\n",
    "img_pre,img_post,id_pre, id_post, dis, dis_type = create_image_tensors(\"/Users/gmeneses/DScourse/00_capstone/xView2_baseline_fork/data4inference/images\",\"/Users/gmeneses/DScourse/00_capstone/xView2_baseline_fork/data4inference/labels\")\n",
    "\n",
    "images_pre = np.stack(img_pre)\n",
    "images_post = np.stack(img_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 1024, 1024, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(images_post)\n",
    "#images_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joplin-tornado', 'lower-puna-volcano', 'moore-tornado',\n",
       "       'nepal-flooding', 'pinery-bushfire', 'portugal-wildfire',\n",
       "       'sunda-tsunami', 'tuscaloosa-tornado', 'woolsey-fire'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 0\n",
    "# image_showcase_post = images_post[n,:,:,:]\n",
    "# print(image_showcase_post.shape)\n",
    "# print(type(image_showcase_post))\n",
    "\n",
    "# disaster = dis[n]\n",
    "# disaster_type=dis_type[n]\n",
    "# print(disaster,disaster_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOADING TENSORS\n",
    "#to recover images and mask arrays:\n",
    "# loaded_arrays_pre = np.load('/Users/gmeneses/DScourse/00_capstone/xView2_baseline_fork/tensors_pre_subset_tensors_pre_xBD_mini.npz')\n",
    "\n",
    "# loaded_arrays_post = np.load('/Users/gmeneses/DScourse/00_capstone/xView2_baseline_fork/tensors_post_subset_tensors_post_xBD_mini.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_pre = loaded_arrays_pre['images']\n",
    "# id = loaded_arrays_pre['id'] # file names for each image-mask pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_post = loaded_arrays_post['images']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a list of tensors\n",
    "#pre_dataset = tf.data.Dataset.from_tensor_slices((images_pre))\n",
    "post_dataset = tf.data.Dataset.from_tensor_slices((images_post))\n",
    "#for only one tensor\n",
    "#image_showcase_post_expanded = tf.expand_dims(image_showcase_post, axis=0)\n",
    "#post_dataset = tf.data.Dataset.from_tensor_slices(image_showcase_post_expanded)\n",
    "\n",
    "#post_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def convert_mask_to_3_channels(mask_1_channel):\n",
    "    \"\"\"convert 1 channel mask (numpy array) in 3 channel mask, preserving labels \n",
    "    defined in \"category_colors\" dictionary\n",
    "    \n",
    "    Args:\n",
    "        mask (~numpy.ndarray): A mask array with 1 dimension.\n",
    "        \n",
    "    Returns:\n",
    "        mask (~numpy.ndarray) with 3 channels.\n",
    "    \"\"\"\n",
    "    # Assuming mask_1_channels has shape (height, width, 1)\n",
    "    height, width, _ = mask_1_channel.shape\n",
    "\n",
    "    # Create an empty array with shape (height, width, 1) for the single-channel mask\n",
    "    deep_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Define the colors representing each category (RGB values)\n",
    "    category_colors = {\n",
    "        0:(0, 0, 0),        # Class 0 - Black (no building) or un-classified\n",
    "        1:(255, 255, 255),  # Class 1 - White (no-damage)\n",
    "        2:(255,255,0),     # Class 2 - Yellow (minor damage)\n",
    "        3:(255,165,0),     # Class 3 - Orange (major damage)\n",
    "        4:(255, 0, 0),     # Class 4 - Red (destroyed)\n",
    "    }\n",
    "    # Loop through each pixel and assign the corresponding category to the single-channel mask\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            pixel_color = mask_1_channel[y, x, 0]\n",
    "            category = category_colors.get(pixel_color, (-1,-1,-1))  # -1 for unknown category\n",
    "            deep_mask[y, x] = category\n",
    "\n",
    "    return deep_mask\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = [\"Input Image\", \"Predicted Mask\"]\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "        plt.axis(\"off\")\n",
    "    # Create the folder if it doesn't exist\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_single(pred_mask):\n",
    "    pred_mask = tf.math.argmax(pred_mask, axis=1)\n",
    "    pred_mask = tf.expand_dims(pred_mask, -1)\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fn(image):\n",
    "    # Assign names to the elements in the dataset\n",
    "    return {\"image\": image}\n",
    "\n",
    "#named_pre_dataset = pre_dataset.map(map_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_post_dataset = post_dataset.map(map_fn)\n",
    "# onefile only\n",
    "#named_post_dataset = map_fn(post_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "\n",
    "image_size = 512\n",
    "mean = tf.constant([0.485, 0.456, 0.406])\n",
    "std = tf.constant([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def normalize(input_image):\n",
    "    input_image = tf.image.convert_image_dtype(input_image, tf.float32)\n",
    "    input_image = (input_image - mean) / tf.maximum(std, backend.epsilon())\n",
    "    return input_image\n",
    "\n",
    "\n",
    "def load_image(datapoint):\n",
    "    input_image = tf.image.resize(datapoint[\"image\"], (image_size, image_size))\n",
    "    input_image = normalize(input_image)\n",
    "    input_image = tf.transpose(input_image, (2, 0, 1))\n",
    "    return {\"pixel_values\": input_image}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pre = (named_pre_dataset.map(load_image))\n",
    "post = (named_post_dataset.map(load_image))\n",
    "# one file only\n",
    "#named_post_dataset[\"image\"]\n",
    "#post = load_image(named_post_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gmeneses/.pyenv/versions/3.11.3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'post' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m#pre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#name_model = \"segmentation_model_test1_last_subset_no_augm\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#images = pre\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[39m#post\u001b[39;00m\n\u001b[1;32m      7\u001b[0m name_model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclassification_model_test1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m images \u001b[39m=\u001b[39m post\n\u001b[1;32m     10\u001b[0m seg_model \u001b[39m=\u001b[39m  TFSegformerForSemanticSegmentation\u001b[39m.\u001b[39mfrom_pretrained(name_model, local_files_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39m#\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'post' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import TFSegformerForSemanticSegmentation\n",
    "#pre\n",
    "#name_model = \"segmentation_model_test1_last_subset_no_augm\"\n",
    "#images = pre\n",
    "\n",
    "#post\n",
    "name_model=\"classification_model_test1\"\n",
    "images = post\n",
    "\n",
    "seg_model =  TFSegformerForSemanticSegmentation.from_pretrained(name_model, local_files_only=True)\n",
    "\n",
    "#\n",
    "for i,elem in enumerate(images):\n",
    "    if dis:\n",
    "        print(dis[i], id_post[i])\n",
    "    #get image tensor\n",
    "    image = elem[\"pixel_values\"]\n",
    "    #print(images_pre.shape)\n",
    "    # add batch dimension, the model needs it\n",
    "    image_batched = np.expand_dims(image, axis=0)\n",
    "    # Use the loaded model to make predictions\n",
    "    pred_seg=seg_model.predict(image_batched).logits\n",
    "    print(\"info prediction\", pred_seg.shape,type(pred_seg))\n",
    "    #convert the predicted mask to \n",
    "    final_pred_seg = create_mask_single(pred_seg)\n",
    "    print(\"info pred after create mask\", final_pred_seg.shape,type(final_pred_seg))\n",
    "    resized_pred_seg=tf.image.resize(final_pred_seg, (512, 512), method=\"nearest\")\n",
    "    print(\"info resized pred\", resized_pred_seg.shape)\n",
    "    #checking if the classes predicted are in the correct range\n",
    "    predicted_flat = np.array(final_pred_seg).astype(int).flatten()\n",
    "    print(\"predicted classes: \",np.unique(predicted_flat))\n",
    "    # # to plot\n",
    "    image_plot = tf.transpose(image, (1, 2, 0))\n",
    "    squeezed_predictions = tf.squeeze(resized_pred_seg, axis=0)  # Remove the batch dimension\n",
    "\n",
    "    pred_mask_create_3ch = convert_mask_to_3_channels(np.array(squeezed_predictions))\n",
    "    display([image_plot, pred_mask_create_3ch])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
